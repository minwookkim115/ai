{"cells":[{"cell_type":"markdown","metadata":{},"source":["# ViT: Vision Transformer\n","- 2020 구글에서 공개한, Transformer을 이미지 처리에 적용한 모델\n","- 이미지 데이터셋의 크기가 클 수록 더욱 좋은 성능을 선보임\n","- Convolution layer 없이 순수 Attention layer로만 모델을 학습하여 좋은 결과를 보임\n","- 전반적으로 BERT와 매우 흡사한 구조"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T12:25:40.110826Z","iopub.status.busy":"2024-09-05T12:25:40.110141Z","iopub.status.idle":"2024-09-05T12:25:40.122370Z","shell.execute_reply":"2024-09-05T12:25:40.121318Z","shell.execute_reply.started":"2024-09-05T12:25:40.110784Z"},"trusted":true},"outputs":[],"source":["import math\n","from random import *\n","import torch.backends\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Modeling\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","import torchvision\n","\n","from sklearn.metrics import accuracy_score\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 모델 구현\n","![](https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-05T12:25:44.420291Z","iopub.status.busy":"2024-09-05T12:25:44.419608Z","iopub.status.idle":"2024-09-05T12:25:44.426252Z","shell.execute_reply":"2024-09-05T12:25:44.425320Z","shell.execute_reply.started":"2024-09-05T12:25:44.420253Z"},"trusted":true},"outputs":[],"source":["# 파라미터 설정\n","img_size = 224   # 입력 이미지 크기\n","patch_size = 16  # 하나의 패치 크기\n","num_patches = (img_size // patch_size) ** 2  # 패치의 개수\n","n_classes = 365  # 분류할 클래스 수\n","\n","n_layers = 12  # 트랜스포머 레이어 수\n","n_heads = 8  # 멀티 헤드 어텐션에서의 헤드 수\n","d_model = 768  # 임베딩 차원\n","d_ff = d_model * 4  # 피드 포워드 네트워크의 차원\n","dropout = 0.1  # 드롭아웃 비율\n","\n","lr = 1e-4   # 학습률 설정\n","batch_size = 16\n","epochs = 5  # 학습할 에폭 수"]},{"cell_type":"markdown","metadata":{},"source":["### 활성화 함수\n","\n","GELU(Gaussian Error Linear Unit)라는 활성화 함수를 구현해봅니다. \n","\n","GELU는 BERT와 같은 트랜스포머 모델에서 자주 사용되며, 다른 활성화 함수보다 깊은 신경망에서 잘 동작하는 것으로 알려져 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def gelu(x):\n","    # Hugging Face에서 구현한 gelu 활성화 함수\n","    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"]},{"cell_type":"markdown","metadata":{},"source":["### Patch Embedding layer 클래스 선언\n","![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png&w=3840&q=75)\n","\n","1. Patch embedding\n","  \n","ViT에서는 이미지를 N x N 크기의 작은 패치로 분할합니다. 예를 들어, 224 x 224 크기의 이미지를 16 x 16 크기의 패치로 분할하면 총 196개의 패치가 생성됩니다. 각 패치의 크기는 16 x 16 x 3(RGB 3채널)을 하나의 벡터로 변환한 후, 이를 임베딩 벡터로 변환합니다. 각 패치는 토큰처럼 취급되며, BERT 모델에서 문장의 단어를 처리하는 방식과 유사합니다.  \n","\n","2. [CLS] token\n","  \n","또한 BERT와 마찬가지로, ViT는 CLS 토큰을 사용합니다. 이 토큰은 패치들 앞에 추가되며, 최종적으로 이미지의 전체적인 표현을 얻고 분류를 하기 위해 사용됩니다.\n","\n","3. Position embedding\n","  \n","트랜스포머 모델은 위치 정보를 내재적으로 처리하지 않기 때문에 패치의 순서 정보가 없으면 학습이 어려워집니다. 이를 해결하기 위해 각 패치에 대한 위치 임베딩이 추가됩니다. 즉, 각 패치가 이미지의 어느 위치에서 나온 것인지를 알려주는 정보가 포함됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 패치 임베딩 클래스 (이미지 패치를 입력받아 벡터로 변환)\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, img_size, patch_size, d_model):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.patch_dim = patch_size * patch_size * 3  # RGB 이미지이므로 3 채널\n","        self.projection = nn.Linear(self.patch_dim, d_model)  # 패치를 d_model 차원으로 변환\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))  # [CLS] 토큰 추가\n","        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))  # 위치 임베딩\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # 입력 이미지 x를 패치로 분할\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, 3, self.img_size, self.img_size)  # (batch_size, 3, img_size, img_size)\n","        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)   # patch_size 대로 이미지를 분할\n","        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(batch_size, -1, self.patch_dim)  # (batch_size, num_patches, patch_dim)\n","\n","        # 패치 임베딩 적용\n","        x = self.projection(x)  # (batch_size, num_patches, d_model)\n","\n","        # [CLS] 토큰을 입력에 추가\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, d_model)\n","        x = torch.cat((cls_tokens, x), dim=1)  # (batch_size, num_patches + 1, d_model)\n","\n","        # 위치 임베딩 추가\n","        x = x + self.pos_embedding\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Multihead attention 클래스 선언\n","\n","트랜스포머 계열의 모델에서 가장 중요한 부분을 꼽으라면, 망설임 없이 멀티 헤드 어텐션 메커니즘이라고 할 수 있습니다.  \n","\n","멀티 헤드 어텐션은 트랜스포머 아키텍처의 핵심 구성 요소 중 하나로, 셀프 어텐션(Self-Attention) 메커니즘을 여러 번 동시에 수행하는 방식입니다.  \n","\n","입력 데이터의 다양한 부분에 주목하여 정보를 집계하는 데 사용되며, 특히나 복잡한 패턴과 관계를 학습하는 데 매우 유용합니다\n","\n","초기화 부분: 쿼리(Q), 키(K), 값(V)에 대한 선형 변환을 정의합니다. 이 변환은 입력 데이터를 여러 '헤드'로 분할하여 각 헤드에서 어텐션을 계산하는 데 사용됩니다.\n","\n","순전파 부분:\n","\n","1. 쿼리, 키, 값에 대한 선형 변환을 수행합니다.\n","\n","2. 멀티 헤드 어텐션을 위해 데이터를 여러 헤드로 분할합니다.\n","\n","3. 각 헤드에서 어텐션 스코어를 계산하고, 필요한 경우 마스크를 적용합니다.\n","\n","4. 어텐션 가중치를 계산하고, 이를 사용하여 값 행렬과 곱하여 어텐션 출력을 얻습니다.\n","5. 모든 헤드의 출력을 연결하고, 추가적인 선형 변환을 수행합니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # 쿼리, 키, 값에 대한 선형 변환\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","\n","        # 드롭아웃 적용\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # 멀티 헤드 어텐션 후의 선형 변환\n","        self.fc = nn.Linear(d_model, d_model)\n","\n","        # 스케일링 팩터\n","        self.scale = torch.sqrt(torch.FloatTensor([d_model // n_heads])).to(device)\n","        # 소프트맥스 함수 정의\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, query, key, value, mask=None):\n","        batch_size = query.shape[0]\n","\n","        # 쿼리, 키, 값에 대한 선형 변환\n","        Q = self.w_q(query)\n","        K = self.w_k(key)\n","        V = self.w_v(value)\n","\n","\n","        # 멀티 헤드 어텐션을 위한 차원 변환\n","        Q = Q.view(batch_size, -1, n_heads, d_model // n_heads).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, n_heads, d_model // n_heads).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, n_heads, d_model // n_heads).permute(0, 2, 1, 3)\n","\n","        # 어텐션 스코어 계산\n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","\n","        # 마스크 적용\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","\n","        # 소프트맥스 함수를 통해 어텐션 가중치 계산\n","        attention = dropout(self.softmax(energy))\n","\n","\n","        # 어텐션 가중치와 값 행렬을 곱하여 출력 계산\n","        x = torch.matmul(attention, V)\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        x = x.view(batch_size, -1, d_model)\n","\n","        # 최종 선형 변환\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["중간에 멀티 헤드 어텐션을 위해 차원을 변환하는 과정이 있습니다.\n","\n","`permute()`는 텐서의 차원을 재배열하는 함수로, 여기서 `permute(0, 2, 1, 3)`를 사용하는 이유는 멀티 헤드 어텐션을 계산하기 위해 텐서의 차원을 적절하게 재배열하기 위함입니다.\n","\n","원래 텐서의 차원은 `[batch_size, seq_len, n_heads, d_model // n_heads]`입니다. 여기서:\n","\n","- `batch_size`: 배치 크기\n","\n","- `seq_len`: 시퀀스 길이\n","\n","- `n_heads`: 어텐션 헤드의 수\n","\n","- `d_model`: 모델의 차원\n","\n","permute(0, 2, 1, 3)를 사용하면 차원의 순서가 [batch_size, n_heads, seq_len, d_model // n_heads]로 변경됩니다.  \n","\n","이렇게 차원을 재배열하면, 각 어텐션 헤드는 독립적으로 시퀀스에 대한 어텐션을 계산할 수 있습니다. 그리고 이렇게 계산된 결과를 다시 원래의 차원 순서로 변경하여 최종 결과를 얻을 수 있습니다.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Positionwise Feedforward Network\n","\n","PositionwiseFeedforward 네트워크는 트랜스포머 아키텍처의 각 인코더와 디코더 레이어에 포함되어 있습니다. 이 네트워크는 기본적으로 두 개의 선형 변환을 연속적으로 적용하는데, 여기서는 1D Convolution을 사용하여 이 변환을 수행합니다.\n","\n","![](https://miro.medium.com/max/1906/1*1l5JbeGfEGh2oxjI8koHdQ.png)\n","\n","초기화 부분: 두 개의 1D Conv 레이어를 정의합니다. 첫 번째 합성곱은 `d_model` 차원의 입력을 `d_ff` 차원으로 확장하고, 두 번째 합성곱은 그 결과를 다시 `d_model` 차원으로 축소합니다.\n","\n","순전파 부분:\n","\n","1. 입력 x의 차원을 변경하여 Convolution을 적용하기 적합한 형태로 만듭니다.\n","\n","2. 첫 번째 합성곱 레이어와 GELU 활성화 함수를 적용한 후, 결과에 드롭아웃을 적용합니다.\n","\n","3. 두 번째 Convolution 레이어를 적용합니다.\n","\n","4. 결과의 차원을 원래대로 변경하여 출력합니다.\n","\n","이 네트워크는 멀티헤드 어텐션의 출력에 비선형 변환을 추가하여 모델의 표현력을 높이는 역할을 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PositionwiseFeedforward(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # 1D Convolution을 사용하여 선형 변환을 수행\n","        self.fc1 = nn.Conv1d(d_model, d_ff, 1)\n","        self.fc2 = nn.Conv1d(d_ff, d_model, 1)\n","\n","        # 드롭아웃 정의\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # 입력 x의 차원을 변경\n","        x = x.permute(0, 2, 1)\n","\n","        # 첫 번째 Convolution과 활성화 함수 적용 후 드롭아웃\n","        x = self.fc1(x)\n","\n","\n","        # 두 번째 Convolution 적용\n","        # Your code\n","\n","        # 차원을 원래대로 변경하여 출력\n","        x = x.permute(0, 2, 1)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder layer\n","\n","트랜스포머 아키텍처의 인코더 레이어를 구현한 클래스입니다.  \n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_22_teJgoCi.max-800x600.png)\n","\n","각 인코더 레이어는 앞서 선언한 두 가지 주요 구성 요소로 이루어져 있습니다:\n","- 멀티 헤드 셀프 어텐션\n","\n","- Position-wise feedforward network\n","\n","초기화 부분:\n","\n","- `MultiHeadAttention`: 멀티헤드 셀프 어텐션을 수행합니다. 이는 입력 시퀀스 내의 각 토큰이 다른 모든 토큰과 어떻게 상호작용하는지를 파악합니다.\n","\n","- `PositionwiseFeedforward`: 네트워크를 통해 추가적인 비선형 변환을 수행합니다.\n","\n","- `LayerNorm`: 레이어 정규화는 각 레이어의 출력을 안정화하여 학습을 도와줍니다.\n","\n","- `Dropout`: 과적합을 방지하기 위한 드롭아웃입니다.\n","\n","순전파 부분:\n","\n","- 멀티헤드 셀프 어텐션을 적용한 후, 그 결과와 원래의 입력을 더하고 레이어 정규화를 수행합니다. (잔여 연결 및 레이어 정규화)\n","\n","- Position-wise Feed forward를 적용한 후, 그 결과와 이전 단계의 출력을 더하고 다시 레이어 정규화를 수행합니다.\n","\n","이 구조는 BERT 인코더의 각 레이어에서 반복적으로 사용되며, 여러 레이어를 쌓아 복잡한 패턴과 관계를 학습할 수 있게 합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 트랜스포머 인코더 레이어 정의\n","class EncoderLayer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.attention = MultiHeadAttention()\n","        self.feed_forward = PositionwiseFeedforward()\n","        self.layer_norm1 = nn.LayerNorm(d_model)\n","        self.layer_norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        # Self-attention\n","        # Your code\n","        \n","        # Add & normalization\n","        # Your code\n","        \"\"\"\n","        1. dropout\n","        2. skip connection\n","        3. layer norm\n","        \"\"\"\n","        \n","        # FFNN\n","        # Your code\n","        \n","        # Add & normalization\n","        # Your code\n","        \"\"\"\n","        1. dropout\n","        2. skip connection\n","        3. layer norm\n","        \"\"\"\n","        return "]},{"cell_type":"markdown","metadata":{},"source":["### ViT 클래스 선언\n","\n","위 내용들을 바탕으로 ViT 모델을 클래스로 선언하겠습니다.  \n","\n","초기화 부분:\n","\n","- `embedding`: 이미지가 패치 단위로 분할 후 임베딩을 거치며 위치 정보와 결합됩니다. [CLS]이 추가되며 벡터 형태로 변환하는 임베딩 레이어입니다.\n","\n","- `encoder_layers`: ViT 모델의 핵심 부분인 인코더 레이어들의 리스트입니다. 각 레이어는 멀티헤드 어텐션과 Position-wise Feedforward 네트워크를 포함합니다.\n","\n","- `linear, activn, norm`: 추가적인 변환을 위한 레이어와 활성화 함수입니다.\n","\n","- classifier: 이미지의 특징을 읽고 클래스로 분류합니다.\n","\n","순전파 부분:\n","\n","- 입력 이미지는 `embedding` 레이어를 통과하여 임베딩 벡터로 변환됩니다.\n","\n","- 임베딩 출력은 순차적으로 각 `encoder_layers`를 통과하며, 각 레이어에서는 멀티헤드 어텐션과 Position-wise Feedforward 연산이 수행됩니다.\n","\n","- 모든 인코더 레이어를 통과한 후, `[CLS]` 토큰의 출력만을 사용하여 이미지를 분류합니다.\n","\n","- 이 표현은 `classifier`를 통과하여 최종 출력을 생성합니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ViT 모델 정의\n","class ViT(nn.Module):\n","    def __init__(self, img_size, patch_size, d_model, n_layers, n_classes):\n","        super().__init__()\n","        self.embedding = PatchEmbedding(img_size, patch_size, d_model)\n","        self.encoder_layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n","        self.classifier = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, n_classes)\n","        )\n","\n","    def forward(self, x):\n","        # 패치 임베딩 통과\n","        # Your code\n","        \n","        # 인코더 레이어 통과\n","        # Your code\n","            \n","        # [CLS] 토큰만 사용하여 최종 분류\n","        cls_token_output = x[:, 0]\n","        # Your code\n","        \n","        return "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모델 생성\n","vit_model = ViT(img_size=img_size, \n","                patch_size=patch_size, \n","                d_model=d_model, \n","                n_layers=n_layers, \n","                n_classes=n_classes).to(device)\n","vit_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 임의의 입력 이미지 (batch_size, 3, img_size, img_size) 생성\n","dummy_input = torch.randn(batch_size, 3, img_size, img_size)\n","\n","output = vit_model(dummy_input.to(device))\n","\n","output.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((224, 224)),  # 이미지 크기 조정\n","    torchvision.transforms.ToTensor(),  # 이미지를 Tensor로 변환\n","    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["places365 = torchvision.datasets.Places365(root=\"./data\", \n","                                           download=True, \n","                                           small=True,\n","                                           transform=transform,\n","                                           split=\"val\"\n","                                           )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(places365.imgs), len(places365.targets)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(places365.classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 데이터셋을 10% 학습용, 5% 검증용으로 나누기\n","train_size = int(0.1 * len(places365))\n","val_size = int(train_size/2)\n","test_size = len(places365) - train_size - val_size\n","train_dataset, validation_dataset, test_dataset = random_split(places365, [train_size, val_size, test_size])\n","\n","# 데이터 로더 설정\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.optim as optim\n","\n","# 손실 함수 정의 (CrossEntropyLoss)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 옵티마이저 정의 (Adam)\n","optimizer = optim.Adam(vit_model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T12:26:08.789569Z","iopub.status.busy":"2024-09-05T12:26:08.788809Z","iopub.status.idle":"2024-09-05T12:26:08.807033Z","shell.execute_reply":"2024-09-05T12:26:08.806048Z","shell.execute_reply.started":"2024-09-05T12:26:08.789529Z"},"trusted":true},"outputs":[],"source":["def train_and_validate(model, train_dataloader, validation_dataloader, epochs, optimizer, criterion, device):\n","    # 전체 에포크만큼 학습 및 검증 반복\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch+1}/{epochs}')\n","        print('-' * 10)\n","\n","        # 학습 모드 설정\n","        model.train()\n","        total_loss = 0\n","        train_preds, train_labels = [], []\n","\n","        # 학습 데이터로 학습 진행\n","        progress_bar = tqdm(train_dataloader, desc='Training', position=0, leave=True)\n","        for step, batch in enumerate(progress_bar):\n","            # 배치 데이터를 디바이스에 할당 (이미지와 레이블)\n","            b_images = batch[0].to(device)\n","            b_labels = batch[1].to(device)\n","\n","            # 그래디언트 초기화\n","            model.zero_grad()\n","\n","            # 모델에 입력 데이터 전달 및 출력 얻기\n","            outputs = model(b_images)\n","\n","            # 손실 계산 (다중 클래스 분류에 적합한 CrossEntropyLoss 사용)\n","            loss = criterion(outputs, b_labels)\n","            total_loss += loss.item()\n","\n","            # 그래디언트 계산\n","            loss.backward()\n","\n","            # 그래디언트 클리핑\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # 파라미터 업데이트\n","            optimizer.step()\n","\n","            # 예측값 및 레이블 저장\n","            _, preds = torch.max(outputs, 1)  # 예측 클래스\n","            train_preds.extend(preds.detach().cpu().numpy().tolist())\n","            train_labels.extend(b_labels.detach().cpu().numpy().tolist())\n","\n","            # 진행 바 업데이트\n","            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","        # 평균 학습 손실 출력\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        print(\"\\nAverage training loss: {0:.2f}\".format(avg_train_loss))\n","\n","        # 학습 정확도 계산 및 출력\n","        train_acc = accuracy_score(train_labels, train_preds)\n","        print(\"Training Accuracy: {0:.2f}\".format(train_acc))\n","\n","        # 검증 모드 설정\n","        model.eval()\n","        total_eval_loss = 0\n","        val_preds, val_labels = [], []\n","\n","        # 검증 데이터로 검증 진행\n","        progress_bar = tqdm(validation_dataloader, desc='Validation', position=0, leave=True)\n","        for batch in progress_bar:\n","            # 배치 데이터를 디바이스에 할당\n","            b_images = batch[0].to(device)\n","            b_labels = batch[1].to(device)\n","\n","            # 그래디언트 계산 비활성화\n","            with torch.no_grad():\n","                # 모델에 입력 데이터 전달 및 출력 얻기\n","                outputs = model(b_images)\n","\n","            # 손실 계산\n","            loss = criterion(outputs, b_labels)\n","            total_eval_loss += loss.item()\n","\n","            # 예측값 및 레이블 저장\n","            _, preds = torch.max(outputs, 1)\n","            val_preds.extend(preds.detach().cpu().numpy().tolist())\n","            val_labels.extend(b_labels.detach().cpu().numpy().tolist())\n","\n","            # 진행 바 업데이트\n","            progress_bar.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","\n","        # 평균 검증 손실 출력\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        print(\"\\nAverage validation loss: {0:.2f}\".format(avg_val_loss))\n","\n","        # 검증 정확도 계산 및 출력\n","        val_acc = accuracy_score(val_labels, val_preds)\n","        print(\"Validation Accuracy: {0:.2f}\".format(val_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-05T12:26:09.728362Z","iopub.status.busy":"2024-09-05T12:26:09.727966Z","iopub.status.idle":"2024-09-05T12:30:43.578128Z","shell.execute_reply":"2024-09-05T12:30:43.577143Z","shell.execute_reply.started":"2024-09-05T12:26:09.728321Z"},"trusted":true},"outputs":[],"source":["# 학습 시작\n","train_and_validate(vit_model, train_loader, val_loader, epochs, optimizer, criterion, device)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5649457,"sourceId":9325351,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
