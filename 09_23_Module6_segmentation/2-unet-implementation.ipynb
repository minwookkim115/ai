{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Multi-class Segmentation과 U-Net 구현하기"]},{"cell_type":"markdown","metadata":{},"source":["## Prep"]},{"cell_type":"markdown","metadata":{},"source":["### 라이브러리"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-22T14:43:58.607270Z","iopub.status.busy":"2024-09-22T14:43:58.606819Z","iopub.status.idle":"2024-09-22T14:44:03.728065Z","shell.execute_reply":"2024-09-22T14:44:03.727239Z","shell.execute_reply.started":"2024-09-22T14:43:58.607218Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from glob import glob\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import torchvision\n","from torchvision import models\n","from torchvision import transforms\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["### 데이터 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:03.730330Z","iopub.status.busy":"2024-09-22T14:44:03.729910Z","iopub.status.idle":"2024-09-22T14:44:03.892018Z","shell.execute_reply":"2024-09-22T14:44:03.891141Z","shell.execute_reply.started":"2024-09-22T14:44:03.730295Z"},"trusted":true},"outputs":[],"source":["image_list = glob('/kaggle/input/car-segmentation/car-segmentation/images/*')\n","\n","mask_list = glob('/kaggle/input/car-segmentation/car-segmentation/masks/*')\n","len(image_list), len(mask_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:03.893557Z","iopub.status.busy":"2024-09-22T14:44:03.893185Z","iopub.status.idle":"2024-09-22T14:44:09.931029Z","shell.execute_reply":"2024-09-22T14:44:09.929885Z","shell.execute_reply.started":"2024-09-22T14:44:03.893514Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n","axes = axes.flatten()\n","\n","for i in range(4):\n","    img_path = image_list[i]\n","    mask_path = image_list[i].replace('images', 'masks')\n","    \n","    axes[2 * i].imshow(Image.open(img_path))\n","    axes[2 * i].set_title(f'Image {i+1}')\n","    axes[2 * i].axis('off')\n","    \n","    axes[2 * i + 1].imshow(Image.open(mask_path))\n","    axes[2 * i + 1].set_title(f'Mask {i+1}')\n","    axes[2 * i + 1].axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["### 클래스 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:09.933365Z","iopub.status.busy":"2024-09-22T14:44:09.933045Z","iopub.status.idle":"2024-09-22T14:44:09.945109Z","shell.execute_reply":"2024-09-22T14:44:09.944221Z","shell.execute_reply.started":"2024-09-22T14:44:09.933331Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/car-segmentation/car-segmentation/classes.txt', 'r') as f:\n","    cls_list = f.read().split(',')\n","    cls_list = [cls.strip() for cls in cls_list]\n","print(cls_list)\n","num_classes = len(cls_list)"]},{"cell_type":"markdown","metadata":{},"source":["### 마스크 고유값 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:09.946932Z","iopub.status.busy":"2024-09-22T14:44:09.946541Z","iopub.status.idle":"2024-09-22T14:44:10.042226Z","shell.execute_reply":"2024-09-22T14:44:10.041228Z","shell.execute_reply.started":"2024-09-22T14:44:09.946899Z"},"trusted":true},"outputs":[],"source":["unique, counts = np.unique(mask_list, return_counts=True)\n","for mask_path in mask_list:\n","    mask = Image.open(mask_path)\n","    mask = np.array(mask)\n","    print(mask)\n","#     print(mask.shape)\n","    print(np.unique(mask))\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["### 이미지-마스크 겹쳐 그리기\n","`torchvision.utils.draw_segmentation_masks`"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:10.043675Z","iopub.status.busy":"2024-09-22T14:44:10.043340Z","iopub.status.idle":"2024-09-22T14:44:10.052755Z","shell.execute_reply":"2024-09-22T14:44:10.051647Z","shell.execute_reply.started":"2024-09-22T14:44:10.043642Z"},"trusted":true},"outputs":[],"source":["def imgs_with_masks(img_dir, start):\n","    img_dir = img_dir[start: start + 4]\n","    fig, axes = plt.subplots(1, 4, figsize=(15, 8))\n","    \n","    for i, img_path in enumerate(img_dir):\n","        # 이미지 및 마스크 불러오기\n","        img = torchvision.io.read_image(img_path)\n","        mask = torchvision.io.read_image(img_path.replace('images', 'masks')).squeeze(0)  # 마스크에서 불필요한 채널 제거\n","\n","        # 마스크를 원-핫 인코딩하여 5개의 클래스로 변환\n","        one_hot_mask = F.one_hot(mask.to(torch.int64), num_classes=num_classes).permute(2, 0, 1).float()\n","        print(img.shape, mask.shape, one_hot_mask.shape)\n","        \n","        # 각 마스크 채널을 True/False로 변환하여 오버레이 생성\n","        overlayed = torchvision.utils.draw_segmentation_masks(img, \n","                                                              one_hot_mask.to(torch.bool), \n","                                                              alpha=0.5, \n","                                                              colors=['black', 'green', 'blue', 'yellow', 'purple'])\n","\n","        axes[i].imshow(overlayed.permute(1, 2, 0))\n","        axes[i].axis('off')\n","        axes[i].set_title(f'Image {i+1}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:10.054465Z","iopub.status.busy":"2024-09-22T14:44:10.054088Z","iopub.status.idle":"2024-09-22T14:44:16.547853Z","shell.execute_reply":"2024-09-22T14:44:16.546759Z","shell.execute_reply.started":"2024-09-22T14:44:10.054421Z"},"trusted":true},"outputs":[],"source":["imgs_with_masks(image_list, 0)"]},{"cell_type":"markdown","metadata":{},"source":["### 3차원이 아닌 이미지 제거"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:16.549782Z","iopub.status.busy":"2024-09-22T14:44:16.549351Z","iopub.status.idle":"2024-09-22T14:44:33.850871Z","shell.execute_reply":"2024-09-22T14:44:33.849841Z","shell.execute_reply.started":"2024-09-22T14:44:16.549741Z"},"trusted":true},"outputs":[],"source":["exclude = []\n","\n","for img_path in image_list:\n","    img = Image.open(img_path)\n","    img = np.array(img)\n","    if img.ndim != 3:\n","        print(img.shape)\n","        print(img.ndim)\n","        print(img_path)\n","        exclude.append(img_path)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:33.852543Z","iopub.status.busy":"2024-09-22T14:44:33.852223Z","iopub.status.idle":"2024-09-22T14:44:33.857149Z","shell.execute_reply":"2024-09-22T14:44:33.856070Z","shell.execute_reply.started":"2024-09-22T14:44:33.852510Z"},"trusted":true},"outputs":[],"source":["for ex in exclude:\n","    image_list.remove(ex)\n","    mask_list.remove(ex.replace('images', 'masks'))"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset\n","- Image: (B, 3, H, W)\n","- Mask: (B, H, W)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:33.860342Z","iopub.status.busy":"2024-09-22T14:44:33.860029Z","iopub.status.idle":"2024-09-22T14:44:33.869004Z","shell.execute_reply":"2024-09-22T14:44:33.868025Z","shell.execute_reply.started":"2024-09-22T14:44:33.860299Z"},"trusted":true},"outputs":[],"source":["class CarDataset(Dataset):\n","    def __init__(self, image_list, mask_list, transform_img, transform_mask, num_classes=num_classes):\n","        \n","        self.image_list = image_list\n","        self.mask_list = mask_list\n","        self.transform_img = transform_img\n","        self.transform_mask = transform_mask\n","        self.num_classes = num_classes\n","        \n","    def __len__(self):\n","        \n","        return len(self.image_list)\n","    \n","    def __getitem__(self, idx):\n","        \n","        image_path = self.image_list[idx]\n","        mask_path = image_path.replace('images', 'masks')\n","        \n","        image = Image.open(image_path)\n","        mask = torchvision.io.read_image(mask_path)\n","        \n","        \n","        # 이미지 Channel=4 -> RGBA\n","        if image.mode == 'RGBA':\n","            image = image.convert('RGB')\n","        \n","        image = self.transform_img(image)\n","        mask = self.transform_mask(mask).squeeze(0).to(torch.long)\n","        \n","        return image, mask"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:44:33.870535Z","iopub.status.busy":"2024-09-22T14:44:33.870203Z","iopub.status.idle":"2024-09-22T14:44:33.882642Z","shell.execute_reply":"2024-09-22T14:44:33.881729Z","shell.execute_reply.started":"2024-09-22T14:44:33.870485Z"},"trusted":true},"outputs":[],"source":["transform_img = transforms.Compose([transforms.ToTensor(), \n","                                    transforms.Resize((224, 224)), \n","                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","                                   ])\n","\n","transform_mask = transforms.Compose([transforms.Resize((224, 224)),\n","                                    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:46:05.363874Z","iopub.status.busy":"2024-09-22T14:46:05.363457Z","iopub.status.idle":"2024-09-22T14:46:06.552254Z","shell.execute_reply":"2024-09-22T14:46:06.551269Z","shell.execute_reply.started":"2024-09-22T14:46:05.363836Z"},"trusted":true},"outputs":[],"source":["car_dataset = CarDataset(image_list, mask_list, transform_img, transform_mask)\n","print(car_dataset.__len__())\n","\n","train_size = int(0.8 * len(car_dataset))\n","val_size = len(car_dataset) - train_size\n","\n","train_ds, val_ds = random_split(car_dataset, [train_size, val_size])\n","print(f\"Train dataset size: {len(train_ds)} | Validation dataset size: {len(val_ds)}\")\n","\n","train_dl = DataLoader(train_ds, batch_size = 16, shuffle = True)\n","val_dl = DataLoader(val_ds, batch_size = 16, shuffle = False)\n","\n","imgs, masks = next(iter(train_dl))\n","print(imgs.shape, masks.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## 학습 및 추론\n","\n","### 모델"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:35:35.818009Z","iopub.status.busy":"2024-09-20T12:35:35.817608Z","iopub.status.idle":"2024-09-20T12:35:35.829285Z","shell.execute_reply":"2024-09-20T12:35:35.828196Z","shell.execute_reply.started":"2024-09-20T12:35:35.817970Z"},"trusted":true},"outputs":[],"source":["class FCN8s(nn.Module):\n","    def __init__(self, n_classes):\n","        super(FCN8s, self).__init__()\n","        vgg = models.vgg16(pretrained=True)\n","        features = list(vgg.features.children())\n","\n","        # VGG16의 각 블록을 PyTorch Sequential로 구성\n","        self.block3 = nn.Sequential(*features[:17])  # Conv1 ~ Conv3\n","        self.block4 = nn.Sequential(*features[17:24])  # Conv4\n","        self.block5 = nn.Sequential(*features[24:])  # Conv5\n","\n","        # 추가 Conv 레이어\n","        self.conv6 = nn.Conv2d(512, 4096, kernel_size=7, padding=3)\n","        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n","\n","        # FCN에서 사용할 1x1 Conv\n","        self.conv1x1_pool3 = nn.Conv2d(256, n_classes, kernel_size=1)\n","        self.conv1x1_pool4 = nn.Conv2d(512, n_classes, kernel_size=1)\n","        self.conv1x1_output = nn.Conv2d(4096, n_classes, kernel_size=1)\n","\n","        # Transposed convolutions for upsampling\n","        self.upconv2 = nn.ConvTranspose2d(n_classes, n_classes, kernel_size=4, stride=2, padding=1)\n","        self.upconv8 = nn.ConvTranspose2d(n_classes, n_classes, kernel_size=8, stride=8, padding=0)\n","\n","    def forward(self, x):\n","        # VGG16 인코더 부분\n","        p3 = self.block3(x)  # (256, H/8, W/8)\n","        p4 = self.block4(p3)  # (512, H/16, W/16)\n","        p5 = self.block5(p4)  # (512, H/32, W/32)\n","        p5 = F.relu(self.conv6(p5))  # (4096, H/32, W/32)\n","        p5 = F.relu(self.conv7(p5))  # (4096, H/32, W/32)\n","\n","        # FCN-8 헤드 부분\n","        score = self.conv1x1_output(p5)\n","        score = self.upconv2(score) + self.conv1x1_pool4(p4)\n","        score = self.upconv2(score) + self.conv1x1_pool3(p3)\n","        output = self.upconv8(score)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:10:34.730314Z","iopub.status.busy":"2024-09-20T12:10:34.729526Z","iopub.status.idle":"2024-09-20T12:10:37.405401Z","shell.execute_reply":"2024-09-20T12:10:37.404495Z","shell.execute_reply.started":"2024-09-20T12:10:34.730267Z"},"trusted":true},"outputs":[],"source":["# 모델 인스턴스 생성\n","model = FCN8s(n_classes=num_classes).to(device)\n","\n","input_image = torch.randn(1, 3, 224, 224).to(device)\n","\n","output = model(input_image)\n","print(output.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### 학습 루프 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:45:02.945322Z","iopub.status.busy":"2024-09-22T14:45:02.944928Z","iopub.status.idle":"2024-09-22T14:45:02.976733Z","shell.execute_reply":"2024-09-22T14:45:02.975469Z","shell.execute_reply.started":"2024-09-22T14:45:02.945284Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(params = model.parameters(), lr = 5e-4)\n","epochs = 15"]},{"cell_type":"markdown","metadata":{},"source":["### 평가지표\n","- mean IoU\n","- mean Pixel Accuracy  \n","두 지표를 다중 클래스 세그멘테이션 모델에 적용하도록 바꿔봅니다."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:45:29.552780Z","iopub.status.busy":"2024-09-22T14:45:29.552127Z","iopub.status.idle":"2024-09-22T14:45:29.561817Z","shell.execute_reply":"2024-09-22T14:45:29.560937Z","shell.execute_reply.started":"2024-09-22T14:45:29.552738Z"},"trusted":true},"outputs":[],"source":["def IoU(output, mask, num_classes):\n","    \"\"\"\n","    1. 각 클래스별로 IoU 계산\n","    2. 클래스별 IoU를 평균하여 최종 IoU 반환\n","    \"\"\"\n","    output = torch.argmax(output, dim=1)  # 각 픽셀에서 가장 높은 값(클래스) 선택\n","    iou_per_class = []\n","    \n","    # Your code\n","    return # 각 클래스별 IoU의 평균\n","\n","def PA(output, mask):\n","    \"\"\"\n","    1. 각 픽셀의 클래스 예측과 실제 클래스의 일치 여부 계산\n","    2. 전체 픽셀 중 일치하는 픽셀의 비율 반환\n","    \"\"\"\n","    output = torch.argmax(output, dim=1)  # 각 픽셀에서 가장 높은 값(클래스) 선택\n","    # Your code\n","    return "]},{"cell_type":"markdown","metadata":{},"source":["### 학습 루프"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:45:30.112195Z","iopub.status.busy":"2024-09-22T14:45:30.111337Z","iopub.status.idle":"2024-09-22T14:45:30.125232Z","shell.execute_reply":"2024-09-22T14:45:30.124299Z","shell.execute_reply.started":"2024-09-22T14:45:30.112153Z"},"trusted":true},"outputs":[],"source":["def train_and_validate(model, train_loader, val_loader, optim, criterion, epochs, num_classes):\n","    train_losses = []\n","    train_IoUs = []\n","    train_PAs = []\n","    val_losses = []\n","    val_IoUs = []\n","    val_PAs = []\n","    \n","    for epoch in range(epochs):\n","        train_loss = 0\n","        train_IoU = 0\n","        train_PA = 0\n","        model.train()\n","        for img, mask in tqdm(train_loader):\n","            img = img.to(device)\n","            mask = mask.to(device)\n","            optim.zero_grad()\n","            output = model(img)\n","            loss = criterion(output, mask)\n","            loss.backward()\n","            optim.step()\n","            train_loss += loss.item()\n","            train_IoU += IoU(output.detach().cpu(), mask.detach().cpu(), num_classes).item()\n","            train_PA += PA(output.detach().cpu(), mask.detach().cpu()).item()\n","            \n","        print(f\"Epoch {epoch + 1}\\nTrain loss: {train_loss / len(train_loader):.2f} | Train IoU: {train_IoU / len(train_loader):.2f} | Train Pixel Acc: {train_PA / len(train_loader):.2f}\")\n","        train_losses.append(train_loss / len(train_loader))\n","        train_IoUs.append(train_IoU / len(train_loader))\n","        train_PAs.append(train_PA / len(train_loader))\n","        \n","        val_loss = 0\n","        val_IoU = 0\n","        val_PA = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for img, mask in tqdm(val_loader):\n","                img = img.to(device)\n","                mask = mask.to(device)\n","                output = model(img)\n","                loss = criterion(output, mask)\n","                val_loss += loss.item()\n","                val_IoU += IoU(output.detach().cpu(), mask.detach().cpu(), num_classes).item()\n","                val_PA += PA(output.detach().cpu(), mask.detach().cpu()).item()\n","                \n","            print(f\"Validation loss: {val_loss / len(val_loader):.2f} | Validation IoU: {val_IoU / len(val_loader):.2f} | Validation Pixel Acc: {val_PA / len(val_loader):.2f}\\n{'='*100}\")\n","            val_losses.append(val_loss / len(val_loader))\n","            val_IoUs.append(val_IoU / len(val_loader))\n","            val_PAs.append(val_PA / len(val_loader))\n","            \n","    return train_losses, train_IoUs, train_PAs, val_losses, val_IoUs, val_PAs\n"]},{"cell_type":"markdown","metadata":{},"source":["### 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:35:39.838492Z","iopub.status.busy":"2024-09-20T12:35:39.838102Z","iopub.status.idle":"2024-09-20T12:37:54.847750Z","shell.execute_reply":"2024-09-20T12:37:54.846605Z","shell.execute_reply.started":"2024-09-20T12:35:39.838450Z"},"trusted":true},"outputs":[],"source":["train_losses, train_IoUs, train_PAs, val_losses, val_IoUs, val_PAs = train_and_validate(model, train_dl, val_dl, optim, criterion, epochs, num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["### 로그 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:38:50.525406Z","iopub.status.busy":"2024-09-20T12:38:50.525007Z","iopub.status.idle":"2024-09-20T12:38:51.315542Z","shell.execute_reply":"2024-09-20T12:38:51.314652Z","shell.execute_reply.started":"2024-09-20T12:38:50.525368Z"},"trusted":true},"outputs":[],"source":["plt.plot(train_losses, label = 'train loss')\n","plt.plot(val_losses, label = 'val loss')\n","plt.legend()\n","plt.show()\n","plt.plot(train_IoUs, label = 'train mIoU')\n","plt.plot(val_IoUs, label = 'val mIoU')\n","plt.legend()\n","plt.show()\n","plt.plot(train_PAs, label = 'train Pixel Accuracy')\n","plt.plot(val_PAs, label = 'val Pixel Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:45:34.672077Z","iopub.status.busy":"2024-09-22T14:45:34.671220Z","iopub.status.idle":"2024-09-22T14:45:34.681234Z","shell.execute_reply":"2024-09-22T14:45:34.680424Z","shell.execute_reply.started":"2024-09-22T14:45:34.672033Z"},"trusted":true},"outputs":[],"source":["def plot_batch(model, data_loader):\n","    model.eval()\n","    with torch.no_grad():\n","        for img, mask in tqdm(data_loader):\n","            img = img.to(device)\n","            output = model(img)\n","            \n","            # output에서 가장 높은 클래스 인덱스 선택 (배치에서 각 픽셀별로 argmax)\n","            output = torch.argmax(output, dim=1).cpu().numpy()  # (batch_size, H, W)\n","            \n","            img = img.cpu().numpy().transpose(0, 2, 3, 1)  # (batch_size, H, W, 3)\n","            mask = mask.cpu().numpy()  # (batch_size, H, W)\n","            break\n","    \n","    for i in range(data_loader.batch_size):\n","        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n","        \n","        # 원본 이미지 시각화\n","        ax[0].imshow(img[i])\n","        ax[0].set_title('image')\n","        \n","        # 마스크 시각화 (vmin=0, vmax=5로 클래스 범위를 설정)\n","        ax[1].imshow(mask[i], cmap='gray', vmin=0, vmax=5)\n","        ax[1].set_title('mask')\n","        \n","        # 예측 마스크 시각화 (vmin=0, vmax=5로 클래스 범위를 설정)\n","        ax[2].imshow(output[i], cmap='gray', vmin=0, vmax=5)\n","        ax[2].set_title('predicted mask')\n","        \n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:38:55.244138Z","iopub.status.busy":"2024-09-20T12:38:55.243709Z","iopub.status.idle":"2024-09-20T12:39:07.394622Z","shell.execute_reply":"2024-09-20T12:39:07.393735Z","shell.execute_reply.started":"2024-09-20T12:38:55.244090Z"},"trusted":true},"outputs":[],"source":["plot_batch(model, val_dl)"]},{"cell_type":"markdown","metadata":{},"source":["## U-net 구현\n","아래 그림과 설명을 따라 custom U-Net을 구현합니다.\n","![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*0qJdnSxkerwZPCWVdLPpHw.png)\n","\n","- UNet은 **Encoder (Contracting Path)**와 **Decoder (Expansive Path)**로 나뉨\n","    - 4번의 Downsampling과 4번의 Upsampling\n","    - Upsampling은 convolutional transpose(kernel_size=2)\n","    - Downsampling는 max pooling(kernel_size=2)\n","- Feature map 크기가 유지되는 부분은 3*3 Conv 활용\n","    - 2번의 Conv layer와 2번의 ReLU\n","- Skip connection 시에는 Feature map을 합연산 하지않고 concat(`torch.cat(dim=1)`) 사용\n","- Decoder의 마지막 부분인 mask prediction에서 1x1 Conv 사용"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:45:17.957972Z","iopub.status.busy":"2024-09-22T14:45:17.957544Z","iopub.status.idle":"2024-09-22T14:45:17.974431Z","shell.execute_reply":"2024-09-22T14:45:17.972947Z","shell.execute_reply.started":"2024-09-22T14:45:17.957935Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        # Your code\n","    \n","    def forward(self, x):\n","        # Your code\n","        return"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:52:31.956035Z","iopub.status.busy":"2024-09-22T14:52:31.955422Z","iopub.status.idle":"2024-09-22T14:52:32.239006Z","shell.execute_reply":"2024-09-22T14:52:32.238043Z","shell.execute_reply.started":"2024-09-22T14:52:31.955988Z"},"trusted":true},"outputs":[],"source":["model = UNet(in_channels=3, out_channels=num_classes).to(device)\n","dummy = torch.randn(1, 3, 224, 224).to(device)\n","output = model(dummy)\n","output.shape"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:52:34.879198Z","iopub.status.busy":"2024-09-22T14:52:34.878411Z","iopub.status.idle":"2024-09-22T14:52:34.884928Z","shell.execute_reply":"2024-09-22T14:52:34.883985Z","shell.execute_reply.started":"2024-09-22T14:52:34.879158Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optim = torch.optim.Adam(params = model.parameters(), lr = 5e-4)\n","epochs = 15"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:55:16.589141Z","iopub.status.busy":"2024-09-22T14:55:16.588370Z","iopub.status.idle":"2024-09-22T14:57:33.029010Z","shell.execute_reply":"2024-09-22T14:57:33.027869Z","shell.execute_reply.started":"2024-09-22T14:55:16.589101Z"},"trusted":true},"outputs":[],"source":["train_losses, train_IoUs, train_PAs, val_losses, val_IoUs, val_PAs = train_and_validate(model, train_dl, val_dl, optim, criterion, epochs, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:57:40.055045Z","iopub.status.busy":"2024-09-22T14:57:40.054254Z","iopub.status.idle":"2024-09-22T14:57:40.745699Z","shell.execute_reply":"2024-09-22T14:57:40.744766Z","shell.execute_reply.started":"2024-09-22T14:57:40.055004Z"},"trusted":true},"outputs":[],"source":["plt.plot(train_losses, label = 'train loss')\n","plt.plot(val_losses, label = 'val loss')\n","plt.legend()\n","plt.show()\n","plt.plot(train_IoUs, label = 'train mIoU')\n","plt.plot(val_IoUs, label = 'val mIoU')\n","plt.legend()\n","plt.show()\n","plt.plot(train_PAs, label = 'train Pixel Accuracy')\n","plt.plot(val_PAs, label = 'val Pixel Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-22T14:57:40.747723Z","iopub.status.busy":"2024-09-22T14:57:40.747389Z","iopub.status.idle":"2024-09-22T14:57:53.392175Z","shell.execute_reply":"2024-09-22T14:57:53.391220Z","shell.execute_reply.started":"2024-09-22T14:57:40.747689Z"},"trusted":true},"outputs":[],"source":["plot_batch(model, val_dl)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1005067,"sourceId":1705856,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
