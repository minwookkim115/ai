{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.quantization\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nfrom tqdm.notebook import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터셋 & 로더","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PTQ(Post-Training static Quantization)\nPTQ에서는 모델을 학습한 후에 양자화를 적용합니다.  \n\n정적 양자화에서는 모델을 양자화하기 전에 칼리브레이션(calibration) 단계를 거칩니다. 이는 일부 입력 데이터를 사용하여 모델 내의 활성화 값의 범위를 추정하는 과정입니다. 이를 바탕으로 정적 양자화는 **가중치(weight)**와 **활성화값(activation)**을 모두 정수형(INT8)으로 변환합니다.","metadata":{}},{"cell_type":"markdown","source":"### CNN 모델 정의\n\n기본 CNN 구조를 사용하되, 양자화를 위해 `QuantStub`과 `DeQuantStub`을 사용합니다. 이 두 모듈을 사용하면 양자화된 데이터가 네트워크에 들어가고, 부동소수점 값으로 복구됩니다.\n- 입력: `QuantStub`에서 양자화되고\n- 출력: `DeQuantStub`에서 다시 부동소수점으로 변환","metadata":{}},{"cell_type":"code","source":"# CNN 모델 정의\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(1600, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(2)\n        \n        # 양자화 준비를 위한 QuantStub 및 DeQuantStub 추가\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # 양자화된 입력값을 받기 위한 QuantStub\n        x = self.quant(x)\n        \n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        \n        # 다시 부동 소수점으로 복구하기 위한 DeQuantStub\n        x = self.dequant(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNNModel().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습 및 추론 함수 정의","metadata":{}},{"cell_type":"code","source":"# 모델 학습 함수\ndef train_model(model, train_loader, optimizer, criterion, device):\n    model.train()\n    for epoch in range(1, 6):  # 5 epochs\n        running_loss = 0.0\n        for data, target in tqdm(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f'Epoch {epoch}, Loss: {running_loss/len(train_loader)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 평가 함수\ndef test_model(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in tqdm(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습\nPTQ에서는 학습이 진행된 후 양자화가 수행된다고 가정합니다.","metadata":{}},{"cell_type":"code","source":"train_model(model, train_loader, optimizer, criterion, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"test_model(model, test_loader, criterion, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quantization\n\n양자화는 CPU 상에서 진행되며, `FBGEMM` Backend를 이용해야 합니다.  \n\n`torch.quantization.get_default_qconfig('fbgemm')`는 CPU에서 INT8 연산을 수행하기 위한 Backend입니다.\n  \n양자화는 train data로 양자화 준비 과정(칼리브레이션)을 거치며, 이는 모델의 활성화값(activation)의 범위를 추정하기 위해서입니다.","metadata":{}},{"cell_type":"code","source":"# 양자화 함수\ndef quantize_model(model, train_loader):\n    # 모델 양자화 준비\n    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n    torch.quantization.prepare(model, inplace=True)\n    \n    # Calibration을 위한 일부 데이터로 모델 실행\n    model.eval()\n    with torch.no_grad():\n        for data, target in tqdm(train_loader):\n            model(data)\n            break  # Calibration을 위한 한 batch만 사용\n            \n    # 양자화 완료\n    torch.quantization.convert(model, inplace=True)\n    print(\"Quantization Complete.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. qconfig 설정\n- `model.qconfig = torch.quantization.get_default_qconfig('fbgemm'))`\n\n- `qconfig`는 양자화의 구성을 정의하는 설정입니다. 여기서는 'fbgemm'을 사용하여 INT8 양자화를 수행하도록 설정합니다.\n\n2. prepare\n- `torch.quantization.prepare(model, inplace=True)`\n- 모델에 양자화를 적용할 준비를 합니다.\n- 양자화를 위한 QuantStub과 DeQuantStub 같은 양자화/비양자화 노드가 삽입되고, 각 레이어에서 양자화 범위를 추적할 수 있게 됩니다.\n\n3. Calibraion\n\n- 모델에 데이터를 통과시켜 활성화값의 범위를 측정합니다.\n\n- 각 레이어에서 활성화값이 차지하는 범위를 추정하며, 추후 양자화에서 활성화값을 INT8로 변환하는 데 중요한 기준으로 사용합니다.\n\n4. convert\n- `torch.quantization.convert(model, inplace=True)`\n- 모델의 가중치와 활성화값이 모두 8비트 정수(INT8)로 변환되며, 부동소수점 연산 대신 정수형 연산을 수행하는 양자화된 모델이 됩니다.","metadata":{}},{"cell_type":"code","source":"model.to('cpu')  # 양자화는 CPU에서 수행\nquantize_model(model, train_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 양자화된 모델 평가\ntest_model(model, test_loader, criterion, 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}