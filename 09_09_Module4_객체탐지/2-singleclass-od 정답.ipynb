{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91366,"sourceType":"datasetVersion","datasetId":50025}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset 준비","metadata":{}},{"cell_type":"code","source":"# !pip install torchsummary -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport PIL\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torchvision import transforms\nimport torchsummary\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/image-localization-dataset/training_images\"\ncount = 0\n\nfor file in os.listdir(path):\n    if file.endswith(\".jpg\"):\n        image_path = os.path.join(path, file)\n        xml_path = os.path.join(path, file.replace(\".jpg\", \".xml\"))\n        if os.path.exists(xml_path) and os.path.exists(image_path) and count < 10:\n            print(f\"image:{image_path}\\nxml:{xml_path}\\n\")\n            count += 1\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XML 파일 읽기 및 구조\n\n`xml.etree.ElementTree` 모듈을 사용하면 XML 파일에서 원하는 데이터를 쉽게 추출할 수 있습니다.  \n  \nXML 파일은 객체 탐지를 위한 라벨링 정보를 포함하고 있습니다. 이 파일에서는 이미지의 경로, 크기, 객체의 이름, 그리고 바운딩 박스 정보(객체의 좌표) 등이 담겨 있습니다. \n\n```xml\n<annotation>\n\t<folder>single cucumber</folder>\n\t<filename>cucumber_31.jpg</filename>\n\t<path>C:\\Users\\Muhammed Buyukkinaci\\Downloads\\single cucumber\\cucumber_31.jpg</path>\n\t<source>\n\t\t<database>Unknown</database>\n\t</source>\n\t<size>\n\t\t<width>227</width>\n\t\t<height>227</height>\n\t\t<depth>3</depth>\n\t</size>\n\t<segmented>0</segmented>\n\t<object>\n\t\t<name>cucumber</name>\n\t\t<pose>Unspecified</pose>\n\t\t<truncated>0</truncated>\n\t\t<difficult>0</difficult>\n\t\t<bndbox>\n\t\t\t<xmin>36</xmin>\n\t\t\t<ymin>11</ymin>\n\t\t\t<xmax>215</xmax>\n\t\t\t<ymax>207</ymax>\n\t\t</bndbox>\n\t</object>\n</annotation>\n```\n\n\n이 중 주목해야 할 주요 요소는 다음과 같습니다.\n\n- `<filename>`: 이미지 파일 이름 (예: cucumber_31.jpg)\n- `<size>`: 이미지의 크기 (width, height, depth)\n- `<object>`: 탐지할 객체의 정보 (예: cucumber)\n- `<bndbox>`: 객체의 좌표 정보 (xmin, ymin, xmax, ymax)","metadata":{}},{"cell_type":"code","source":"xml_path = \"/kaggle/input/image-localization-dataset/training_images/eggplant_35.xml\"\n\n# XML 파싱\ntree = ET.parse(xml_path)\nroot = tree.getroot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 파일 이름, 경로, 이미지 크기 정보 추출\nfilename = root.find('filename').text\nimage_path = root.find('path').text\nsize = root.find('size')\nwidth = int(size.find('width').text)\nheight = int(size.find('height').text)\n\nprint(f\"filename: {filename}\\nimage_path: {image_path}\\nwidth: {width}\\nheight: {height}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 객체 정보 추출\nobj = root.find('object')\nlabel = obj.find('name').text\nbndbox = obj.find('bndbox')\nxmin = int(bndbox.find('xmin').text)\nymin = int(bndbox.find('ymin').text)\nxmax = int(bndbox.find('xmax').text)\nymax = int(bndbox.find('ymax').text)\n\nprint(f\"label: {label}\\nxmin: {xmin}\\nymin: {ymin}\\nxmax: {xmax}\\nymax: {ymax}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 데이터 확인","metadata":{}},{"cell_type":"code","source":"image_path = \"/kaggle/input/image-localization-dataset/training_images/eggplant_35.jpg\"\n\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.imshow(image)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_bbox_cywh_normalized(image_path):\n    \"\"\"\n    이미지 경로 입력 시 xml 파일을 불러오고, bbox 정보를 추출하여 이미지+bbox를 그려주는 함수\n    \"\"\"\n    \n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    xml_path = image_path.replace(\".jpg\", \".xml\")\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    \n    obj = root.find('object')\n    bndbox = obj.find('bndbox')\n    xmin = int(bndbox.find('xmin').text)\n    ymin = int(bndbox.find('ymin').text)\n    xmax = int(bndbox.find('xmax').text)\n    ymax = int(bndbox.find('ymax').text)\n    \n    image = cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n    \n    plt.imshow(image)\n    plt.show()\n    \ndraw_bbox_cywh_normalized(\"/kaggle/input/image-localization-dataset/training_images/eggplant_35.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaled Bounding Box로 이미지 그리기","metadata":{}},{"cell_type":"code","source":"class2label = {\"mushroom\": 0, \"eggplant\": 1, \"cucumber\": 2}\nlabel2class = {v: k for k, v in class2label.items()}\n\nsample_bbox = torch.tensor([xmin, ymin, xmax, ymax]).float() / torch.tensor([width, height, width, height]).float()\n\nsample_label = torch.tensor(class2label[label])\n\nprint(f\"bbox: {sample_bbox}\\nlabel: {sample_label}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_bbox * torch.tensor([width, height, width, height]).float()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_bbox(image, bbox, label):\n    \"\"\"\n    이미지 및 바운딩 박스를 그려주는 함수\n    \"\"\"\n    # 바운딩 박스 좌표를 이미지 크기에 맞게 스케일링하고 정수형으로 변환\n    width, height = image.shape[1], image.shape[0]\n    bbox = (bbox * torch.tensor([width, height, width, height]).float()).numpy().astype(int)\n    print(bbox)\n    image = cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n    \n    # 클래스 라벨을 타이틀로 설정\n    plt.title(label2class[int(label)])\n    plt.imshow(image)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_bbox(image, sample_bbox, sample_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset & DataLoader 구성","metadata":{}},{"cell_type":"code","source":"class customDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_files = []\n        self.annotation_files = []\n        self.classes = {\"mushroom\": 0, \"eggplant\": 1, \"cucumber\": 2}\n        \n        # 이미지와 XML 파일을 쌍으로 추출하여 정리\n        \n        for file in os.listdir(self.root_dir):\n            if file.endswith(\".jpg\"):\n                image_path = os.path.join(self.root_dir, file)\n                xml_path = os.path.join(self.root_dir, file.replace(\".jpg\", \".xml\"))\n                if os.path.exists(xml_path) and os.path.exists(image_path):\n                    self.image_files.append(image_path)\n                    self.annotation_files.append(xml_path)\n                    \n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # 이미지 파일과 XML 파일 경로\n        image_path = self.image_files[idx]\n        xml_path = self.annotation_files[idx]\n        \n        # 이미지 읽기\n        image = PIL.Image.open(image_path)\n\n        # XML 파일에서 바운딩 박스 정보 추출\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n\n        # 객체 정보 추출\n        obj = root.find('object')\n        label = obj.find('name').text\n        label = self.classes[label]\n        \n        bndbox = obj.find('bndbox')\n        xmin = int(bndbox.find('xmin').text) / image.width\n        ymin = int(bndbox.find('ymin').text) / image.height\n        xmax = int(bndbox.find('xmax').text) / image.width\n        ymax = int(bndbox.find('ymax').text) / image.height\n\n        bbox = torch.tensor([xmin, ymin, xmax, ymax]).float()\n        label = torch.tensor(label)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, bbox, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = customDataset(\"/kaggle/input/image-localization-dataset/training_images\", transform=transform)\ntrain_dataset, val_dataset = random_split(dataset, [0.8, 0.2])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset), len(val_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 모델링","metadata":{}},{"cell_type":"code","source":"class detector(nn.Module):\n    def __init__(self, num_classes):\n        super(detector, self).__init__()\n        \n        # CNN Layer\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        \n        # Fully Connected Layer\n        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        \n        # Output layers: 4 values for bounding box (xmin, ymin, xmax, ymax) and class prediction\n        self.fc_bbox = nn.Linear(64, 4)  # 바운딩 박스 좌표 예측\n        self.fc_class = nn.Linear(64, num_classes)  # 클래스 예측\n    \n    def forward(self, x):\n        # Convolutional layers with ReLU and MaxPooling\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        \n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        \n        x = F.relu(self.conv3(x))\n        x = F.max_pool2d(x, 2)\n        \n        # Flattening the tensor for Fully Connected layer\n        x = x.view(x.size(0), -1)  # Flatten\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        \n        # Bounding box prediction (4 values) and class prediction (num_classes values)\n        bbox = self.fc_bbox(x)\n        class_logits = self.fc_class(x)\n        \n        return bbox, class_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모델 초기화\nnum_classes = 3  # cucumber, eggplant, mushroom 3개의 클래스\nmodel = detector(num_classes=num_classes).to(device)\ntorchsummary.summary(model, (3, 224, 224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 손실 함수 설정 (MSE for bbox, CrossEntropy for class)\ncriterion_bbox = nn.MSELoss()  # 바운딩 박스 좌표 예측을 위한 MSE\ncriterion_class = nn.CrossEntropyLoss()  # 클래스 분류를 위한 CrossEntropy\n\n# 옵티마이저 설정\noptimizer = optim.Adam(model.parameters(), lr=0.005)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs, criterion_bbox, criterion_class, optimizer, device):\n    model.to(device)\n    \n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_bbox_loss = 0.0\n        running_class_loss = 0.0\n        \n        # Train\n        for images, bboxes, labels in tqdm(train_loader):\n            images = images.to(device)\n            bboxes = bboxes.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n \n            outputs_bbox, outputs_class = model(images)\n            \n            # 손실 계산\n            loss_bbox = criterion_bbox(outputs_bbox, bboxes)\n            loss_class = criterion_class(outputs_class, labels)\n            loss_total = loss_bbox + loss_class\n            \n            # 역전파 및 옵티마이저 업데이트\n            loss_total.backward()\n            optimizer.step()\n            \n            # 배치의 손실 누적\n            running_loss += loss_total.item()\n            running_bbox_loss += loss_bbox.item()\n            running_class_loss += loss_class.item()\n        \n        # 에포크 당 훈련 손실 계산\n        epoch_loss = running_loss / len(train_loader)\n        epoch_bbox_loss = running_bbox_loss / len(train_loader)\n        epoch_class_loss = running_class_loss / len(train_loader)\n        \n        train_losses.append(epoch_loss)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train bbox Loss: {epoch_bbox_loss:.4f}, Train class Loss: {epoch_class_loss:.4f}\\nTrain Toal Loss: {epoch_loss:.4f}\")\n        \n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n        val_running_bbox_loss = 0.0\n        val_running_class_loss = 0.0\n        \n        with torch.no_grad():\n            for images, bboxes, labels in val_loader:\n                images = images.to(device)\n                bboxes = bboxes.to(device)\n                labels = labels.to(device)\n                \n                outputs_bbox, outputs_class = model(images)\n                \n                # 손실 계산\n                loss_bbox = criterion_bbox(outputs_bbox, bboxes)\n                loss_class = criterion_class(outputs_class, labels)\n                loss_total = loss_bbox + loss_class\n                \n                val_running_loss += loss_total.item()\n                val_running_bbox_loss += loss_bbox.item()\n                val_running_class_loss += loss_class.item()\n        \n        # 에포크 당 검증 손실 계산\n        val_loss = val_running_loss / len(val_loader)\n        val_bbox_loss = val_running_bbox_loss / len(val_loader)\n        val_class_loss = val_running_class_loss / len(val_loader)\n\n        val_losses.append(val_loss)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Val bbox Loss: {val_bbox_loss:.4f}, Val class Loss: {val_class_loss:.4f}\\nVal Total Loss: {val_loss:.4f}\")\n    \n    return train_losses, val_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\ntrain_losses, val_losses = train_model(model, train_loader, val_loader, epochs, criterion_bbox, criterion_class, optimizer, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_losses, label='train loss')\nplt.plot(val_losses, label='val loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, image):\n    model.eval()\n    image = image.to(device)\n    output = model(image.unsqueeze(0))\n    print(output)\n    bbox = output[0][0].detach().cpu()\n    class_logits = output[1][0].detach().cpu()\n    class_prob = F.softmax(class_logits, dim=-1)\n    return bbox, class_prob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox, class_prob = predict(model, val_dataset[0][0])\nbbox, class_prob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = val_dataset[0][0].detach().cpu().numpy().transpose(1, 2, 0)\n# denormalize\nxx = ((xx * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))*255).astype(np.uint8)\n\nplt.imshow(xx)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_prediction(image, bbox, class_prob):\n    image = image.detach().cpu().numpy().transpose(1, 2, 0)\n    image = ((image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))*255).astype(np.uint8)\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    class_prob = class_prob.argmax(axis=-1)\n    print(bbox, class_prob)\n    predicted_img = draw_bbox(image, bbox, class_prob)\n    return predicted_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_prediction(val_dataset[0][0], bbox, class_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}