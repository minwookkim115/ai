{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "mnist = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "mnist_data = mnist.data\n",
    "mnist_target_sub = mnist.target\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "mnist_target = ohe.fit_transform(mnist_target_sub.reshape(-1, 1))\n",
    "\n",
    "x_train_sub = torch.Tensor(mnist_data[:round(len(mnist_data) * 0.6)])\n",
    "y_train_sub = torch.Tensor(mnist_target[:round(len(mnist_target) * 0.6)])\n",
    "\n",
    "x_test_sub = torch.Tensor(mnist_data[round(len(mnist_data) * 0.2):])\n",
    "y_test_sub = torch.Tensor(mnist_target[round(len(mnist_target) * 0.2):])\n",
    "\n",
    "x_val_sub = torch.Tensor(mnist_data[round(len(mnist_data) * 0.6):round(len(mnist_data) * 0.6) + round(len(mnist_data) * 0.2)])\n",
    "y_val_sub = torch.Tensor(mnist_target[round(len(mnist_target) * 0.6):round(len(mnist_target) * 0.6) + round(len(mnist_target) * 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datase = TensorDataset(x_train_sub, y_train_sub)\n",
    "test_datase = TensorDataset(x_test_sub, y_test_sub)\n",
    "val_datase = TensorDataset(x_val_sub, y_val_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_datase, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(mnist_model, self).__init__()\n",
    "        self.lin1 = nn.Linear(64, 32)\n",
    "        self.lin2 = nn.Linear(32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mnist_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Batch 1/68 loss 3.5343668460845947\n",
      "Epoch 1/20 Batch 2/68 loss 3.3237380981445312\n",
      "Epoch 1/20 Batch 3/68 loss 2.6990580558776855\n",
      "Epoch 1/20 Batch 4/68 loss 2.796438455581665\n",
      "Epoch 1/20 Batch 5/68 loss 3.20442533493042\n",
      "Epoch 1/20 Batch 6/68 loss 2.9708218574523926\n",
      "Epoch 1/20 Batch 7/68 loss 2.1921262741088867\n",
      "Epoch 1/20 Batch 8/68 loss 2.429425001144409\n",
      "Epoch 1/20 Batch 9/68 loss 2.183558464050293\n",
      "Epoch 1/20 Batch 10/68 loss 2.2956702709198\n",
      "Epoch 1/20 Batch 11/68 loss 2.0661556720733643\n",
      "Epoch 1/20 Batch 12/68 loss 2.4677438735961914\n",
      "Epoch 1/20 Batch 13/68 loss 2.2454192638397217\n",
      "Epoch 1/20 Batch 14/68 loss 2.716817617416382\n",
      "Epoch 1/20 Batch 15/68 loss 2.322272777557373\n",
      "Epoch 1/20 Batch 16/68 loss 2.037153482437134\n",
      "Epoch 1/20 Batch 17/68 loss 2.0255231857299805\n",
      "Epoch 1/20 Batch 18/68 loss 2.262986183166504\n",
      "Epoch 1/20 Batch 19/68 loss 2.1233181953430176\n",
      "Epoch 1/20 Batch 20/68 loss 2.0190696716308594\n",
      "Epoch 1/20 Batch 21/68 loss 2.1339542865753174\n",
      "Epoch 1/20 Batch 22/68 loss 1.9694924354553223\n",
      "Epoch 1/20 Batch 23/68 loss 1.7927589416503906\n",
      "Epoch 1/20 Batch 24/68 loss 2.0415706634521484\n",
      "Epoch 1/20 Batch 25/68 loss 1.7528870105743408\n",
      "Epoch 1/20 Batch 26/68 loss 1.8341730833053589\n",
      "Epoch 1/20 Batch 27/68 loss 1.945048213005066\n",
      "Epoch 1/20 Batch 28/68 loss 2.0986478328704834\n",
      "Epoch 1/20 Batch 29/68 loss 2.0727479457855225\n",
      "Epoch 1/20 Batch 30/68 loss 1.8252527713775635\n",
      "Epoch 1/20 Batch 31/68 loss 1.595406413078308\n",
      "Epoch 1/20 Batch 32/68 loss 1.690542459487915\n",
      "Epoch 1/20 Batch 33/68 loss 1.9233431816101074\n",
      "Epoch 1/20 Batch 34/68 loss 1.5141053199768066\n",
      "Epoch 1/20 Batch 35/68 loss 1.8418307304382324\n",
      "Epoch 1/20 Batch 36/68 loss 1.5784282684326172\n",
      "Epoch 1/20 Batch 37/68 loss 1.555654525756836\n",
      "Epoch 1/20 Batch 38/68 loss 1.6086735725402832\n",
      "Epoch 1/20 Batch 39/68 loss 1.5352143049240112\n",
      "Epoch 1/20 Batch 40/68 loss 1.6107096672058105\n",
      "Epoch 1/20 Batch 41/68 loss 1.5374091863632202\n",
      "Epoch 1/20 Batch 42/68 loss 1.8142045736312866\n",
      "Epoch 1/20 Batch 43/68 loss 1.6269497871398926\n",
      "Epoch 1/20 Batch 44/68 loss 1.5244345664978027\n",
      "Epoch 1/20 Batch 45/68 loss 1.5102097988128662\n",
      "Epoch 1/20 Batch 46/68 loss 1.4145630598068237\n",
      "Epoch 1/20 Batch 47/68 loss 1.652781367301941\n",
      "Epoch 1/20 Batch 48/68 loss 1.5250779390335083\n",
      "Epoch 1/20 Batch 49/68 loss 1.5312209129333496\n",
      "Epoch 1/20 Batch 50/68 loss 1.212378740310669\n",
      "Epoch 1/20 Batch 51/68 loss 1.5317020416259766\n",
      "Epoch 1/20 Batch 52/68 loss 1.2747600078582764\n",
      "Epoch 1/20 Batch 53/68 loss 1.3569144010543823\n",
      "Epoch 1/20 Batch 54/68 loss 1.5284801721572876\n",
      "Epoch 1/20 Batch 55/68 loss 1.4282991886138916\n",
      "Epoch 1/20 Batch 56/68 loss 1.3056707382202148\n",
      "Epoch 1/20 Batch 57/68 loss 1.085367202758789\n",
      "Epoch 1/20 Batch 58/68 loss 1.5295003652572632\n",
      "Epoch 1/20 Batch 59/68 loss 1.3075213432312012\n",
      "Epoch 1/20 Batch 60/68 loss 1.5886406898498535\n",
      "Epoch 1/20 Batch 61/68 loss 1.383910059928894\n",
      "Epoch 1/20 Batch 62/68 loss 1.460180401802063\n",
      "Epoch 1/20 Batch 63/68 loss 1.1044623851776123\n",
      "Epoch 1/20 Batch 64/68 loss 0.9166651964187622\n",
      "Epoch 1/20 Batch 65/68 loss 1.2682560682296753\n",
      "Epoch 1/20 Batch 66/68 loss 1.1190834045410156\n",
      "Epoch 1/20 Batch 67/68 loss 1.2381559610366821\n",
      "Epoch 1/20 Batch 68/68 loss 1.0761610269546509\n",
      "Epoch 2/20 Batch 1/68 loss 1.1319758892059326\n",
      "Epoch 2/20 Batch 2/68 loss 1.060010313987732\n",
      "Epoch 2/20 Batch 3/68 loss 1.377389669418335\n",
      "Epoch 2/20 Batch 4/68 loss 1.1741658449172974\n",
      "Epoch 2/20 Batch 5/68 loss 0.9613872170448303\n",
      "Epoch 2/20 Batch 6/68 loss 1.0704904794692993\n",
      "Epoch 2/20 Batch 7/68 loss 1.2723023891448975\n",
      "Epoch 2/20 Batch 8/68 loss 0.8526981472969055\n",
      "Epoch 2/20 Batch 9/68 loss 1.0305087566375732\n",
      "Epoch 2/20 Batch 10/68 loss 1.0249794721603394\n",
      "Epoch 2/20 Batch 11/68 loss 0.7380968332290649\n",
      "Epoch 2/20 Batch 12/68 loss 1.0504162311553955\n",
      "Epoch 2/20 Batch 13/68 loss 1.1268314123153687\n",
      "Epoch 2/20 Batch 14/68 loss 1.0805816650390625\n",
      "Epoch 2/20 Batch 15/68 loss 0.8290420770645142\n",
      "Epoch 2/20 Batch 16/68 loss 0.7278573513031006\n",
      "Epoch 2/20 Batch 17/68 loss 0.7722710967063904\n",
      "Epoch 2/20 Batch 18/68 loss 1.1214219331741333\n",
      "Epoch 2/20 Batch 19/68 loss 1.009993314743042\n",
      "Epoch 2/20 Batch 20/68 loss 0.919079601764679\n",
      "Epoch 2/20 Batch 21/68 loss 0.9005510210990906\n",
      "Epoch 2/20 Batch 22/68 loss 0.763476550579071\n",
      "Epoch 2/20 Batch 23/68 loss 0.704754114151001\n",
      "Epoch 2/20 Batch 24/68 loss 0.7174431085586548\n",
      "Epoch 2/20 Batch 25/68 loss 0.823952853679657\n",
      "Epoch 2/20 Batch 26/68 loss 0.7750844955444336\n",
      "Epoch 2/20 Batch 27/68 loss 0.7711069583892822\n",
      "Epoch 2/20 Batch 28/68 loss 0.7720848321914673\n",
      "Epoch 2/20 Batch 29/68 loss 0.7038170099258423\n",
      "Epoch 2/20 Batch 30/68 loss 0.9819207191467285\n",
      "Epoch 2/20 Batch 31/68 loss 0.7881313562393188\n",
      "Epoch 2/20 Batch 32/68 loss 1.0464138984680176\n",
      "Epoch 2/20 Batch 33/68 loss 0.6655638217926025\n",
      "Epoch 2/20 Batch 34/68 loss 0.5972945690155029\n",
      "Epoch 2/20 Batch 35/68 loss 0.6700562834739685\n",
      "Epoch 2/20 Batch 36/68 loss 0.6453583240509033\n",
      "Epoch 2/20 Batch 37/68 loss 1.101949691772461\n",
      "Epoch 2/20 Batch 38/68 loss 0.5764865875244141\n",
      "Epoch 2/20 Batch 39/68 loss 0.9210704565048218\n",
      "Epoch 2/20 Batch 40/68 loss 0.9099407196044922\n",
      "Epoch 2/20 Batch 41/68 loss 0.8698076605796814\n",
      "Epoch 2/20 Batch 42/68 loss 0.7461807727813721\n",
      "Epoch 2/20 Batch 43/68 loss 0.5854293704032898\n",
      "Epoch 2/20 Batch 44/68 loss 0.7454982399940491\n",
      "Epoch 2/20 Batch 45/68 loss 0.684496283531189\n",
      "Epoch 2/20 Batch 46/68 loss 0.6220987439155579\n",
      "Epoch 2/20 Batch 47/68 loss 0.6889564394950867\n",
      "Epoch 2/20 Batch 48/68 loss 0.5509887933731079\n",
      "Epoch 2/20 Batch 49/68 loss 0.5916746258735657\n",
      "Epoch 2/20 Batch 50/68 loss 0.6739932298660278\n",
      "Epoch 2/20 Batch 51/68 loss 0.5172164440155029\n",
      "Epoch 2/20 Batch 52/68 loss 0.7693600058555603\n",
      "Epoch 2/20 Batch 53/68 loss 0.5612478256225586\n",
      "Epoch 2/20 Batch 54/68 loss 0.821501612663269\n",
      "Epoch 2/20 Batch 55/68 loss 0.3532959520816803\n",
      "Epoch 2/20 Batch 56/68 loss 0.6005997657775879\n",
      "Epoch 2/20 Batch 57/68 loss 0.9364956617355347\n",
      "Epoch 2/20 Batch 58/68 loss 0.6100863814353943\n",
      "Epoch 2/20 Batch 59/68 loss 0.6492341756820679\n",
      "Epoch 2/20 Batch 60/68 loss 0.7824386358261108\n",
      "Epoch 2/20 Batch 61/68 loss 0.5122615694999695\n",
      "Epoch 2/20 Batch 62/68 loss 0.5641176104545593\n",
      "Epoch 2/20 Batch 63/68 loss 0.43378376960754395\n",
      "Epoch 2/20 Batch 64/68 loss 0.5501036047935486\n",
      "Epoch 2/20 Batch 65/68 loss 0.39202597737312317\n",
      "Epoch 2/20 Batch 66/68 loss 0.527794361114502\n",
      "Epoch 2/20 Batch 67/68 loss 0.6323522329330444\n",
      "Epoch 2/20 Batch 68/68 loss 0.6459748148918152\n",
      "Epoch 3/20 Batch 1/68 loss 0.320264607667923\n",
      "Epoch 3/20 Batch 2/68 loss 0.37367767095565796\n",
      "Epoch 3/20 Batch 3/68 loss 0.4588809013366699\n",
      "Epoch 3/20 Batch 4/68 loss 0.4203526973724365\n",
      "Epoch 3/20 Batch 5/68 loss 0.5453984141349792\n",
      "Epoch 3/20 Batch 6/68 loss 0.3500019609928131\n",
      "Epoch 3/20 Batch 7/68 loss 0.6221780776977539\n",
      "Epoch 3/20 Batch 8/68 loss 0.7007656097412109\n",
      "Epoch 3/20 Batch 9/68 loss 0.35687556862831116\n",
      "Epoch 3/20 Batch 10/68 loss 0.5176989436149597\n",
      "Epoch 3/20 Batch 11/68 loss 0.31116434931755066\n",
      "Epoch 3/20 Batch 12/68 loss 0.6003902554512024\n",
      "Epoch 3/20 Batch 13/68 loss 0.4993870258331299\n",
      "Epoch 3/20 Batch 14/68 loss 0.29522863030433655\n",
      "Epoch 3/20 Batch 15/68 loss 0.4382164478302002\n",
      "Epoch 3/20 Batch 16/68 loss 0.3853190243244171\n",
      "Epoch 3/20 Batch 17/68 loss 0.4515253007411957\n",
      "Epoch 3/20 Batch 18/68 loss 0.41503429412841797\n",
      "Epoch 3/20 Batch 19/68 loss 0.3131003677845001\n",
      "Epoch 3/20 Batch 20/68 loss 0.3706391155719757\n",
      "Epoch 3/20 Batch 21/68 loss 0.41358256340026855\n",
      "Epoch 3/20 Batch 22/68 loss 0.30564355850219727\n",
      "Epoch 3/20 Batch 23/68 loss 0.30691540241241455\n",
      "Epoch 3/20 Batch 24/68 loss 0.258842796087265\n",
      "Epoch 3/20 Batch 25/68 loss 0.4326182007789612\n",
      "Epoch 3/20 Batch 26/68 loss 0.2326412796974182\n",
      "Epoch 3/20 Batch 27/68 loss 0.3269764184951782\n",
      "Epoch 3/20 Batch 28/68 loss 0.28061941266059875\n",
      "Epoch 3/20 Batch 29/68 loss 0.26666486263275146\n",
      "Epoch 3/20 Batch 30/68 loss 0.3168306350708008\n",
      "Epoch 3/20 Batch 31/68 loss 0.2596767842769623\n",
      "Epoch 3/20 Batch 32/68 loss 0.42594701051712036\n",
      "Epoch 3/20 Batch 33/68 loss 0.3282853960990906\n",
      "Epoch 3/20 Batch 34/68 loss 0.5405157208442688\n",
      "Epoch 3/20 Batch 35/68 loss 0.4088544547557831\n",
      "Epoch 3/20 Batch 36/68 loss 0.3581845164299011\n",
      "Epoch 3/20 Batch 37/68 loss 0.3884900212287903\n",
      "Epoch 3/20 Batch 38/68 loss 0.29960691928863525\n",
      "Epoch 3/20 Batch 39/68 loss 0.24355508387088776\n",
      "Epoch 3/20 Batch 40/68 loss 0.418039470911026\n",
      "Epoch 3/20 Batch 41/68 loss 0.2610700726509094\n",
      "Epoch 3/20 Batch 42/68 loss 0.42903006076812744\n",
      "Epoch 3/20 Batch 43/68 loss 0.4030897319316864\n",
      "Epoch 3/20 Batch 44/68 loss 0.3547338545322418\n",
      "Epoch 3/20 Batch 45/68 loss 0.2217518836259842\n",
      "Epoch 3/20 Batch 46/68 loss 0.2918260395526886\n",
      "Epoch 3/20 Batch 47/68 loss 0.22915127873420715\n",
      "Epoch 3/20 Batch 48/68 loss 0.2964584231376648\n",
      "Epoch 3/20 Batch 49/68 loss 0.2894584834575653\n",
      "Epoch 3/20 Batch 50/68 loss 0.21868246793746948\n",
      "Epoch 3/20 Batch 51/68 loss 0.3420811891555786\n",
      "Epoch 3/20 Batch 52/68 loss 0.2202257364988327\n",
      "Epoch 3/20 Batch 53/68 loss 0.16322818398475647\n",
      "Epoch 3/20 Batch 54/68 loss 0.322717547416687\n",
      "Epoch 3/20 Batch 55/68 loss 0.5813300609588623\n",
      "Epoch 3/20 Batch 56/68 loss 0.32315242290496826\n",
      "Epoch 3/20 Batch 57/68 loss 0.33703672885894775\n",
      "Epoch 3/20 Batch 58/68 loss 0.15990646183490753\n",
      "Epoch 3/20 Batch 59/68 loss 0.7703907489776611\n",
      "Epoch 3/20 Batch 60/68 loss 0.4812958538532257\n",
      "Epoch 3/20 Batch 61/68 loss 0.24457181990146637\n",
      "Epoch 3/20 Batch 62/68 loss 0.2198689579963684\n",
      "Epoch 3/20 Batch 63/68 loss 0.08252449333667755\n",
      "Epoch 3/20 Batch 64/68 loss 0.27987509965896606\n",
      "Epoch 3/20 Batch 65/68 loss 0.5499178171157837\n",
      "Epoch 3/20 Batch 66/68 loss 0.297348290681839\n",
      "Epoch 3/20 Batch 67/68 loss 0.1828809231519699\n",
      "Epoch 3/20 Batch 68/68 loss 0.36757218837738037\n",
      "Epoch 4/20 Batch 1/68 loss 0.12792256474494934\n",
      "Epoch 4/20 Batch 2/68 loss 0.13125810027122498\n",
      "Epoch 4/20 Batch 3/68 loss 0.13814546167850494\n",
      "Epoch 4/20 Batch 4/68 loss 0.20002976059913635\n",
      "Epoch 4/20 Batch 5/68 loss 0.37049201130867004\n",
      "Epoch 4/20 Batch 6/68 loss 0.17732471227645874\n",
      "Epoch 4/20 Batch 7/68 loss 0.21110422909259796\n",
      "Epoch 4/20 Batch 8/68 loss 0.1305844783782959\n",
      "Epoch 4/20 Batch 9/68 loss 0.3492807149887085\n",
      "Epoch 4/20 Batch 10/68 loss 0.16302742063999176\n",
      "Epoch 4/20 Batch 11/68 loss 0.3790532946586609\n",
      "Epoch 4/20 Batch 12/68 loss 0.11698046326637268\n",
      "Epoch 4/20 Batch 13/68 loss 0.20296548306941986\n",
      "Epoch 4/20 Batch 14/68 loss 0.3140188455581665\n",
      "Epoch 4/20 Batch 15/68 loss 0.15888264775276184\n",
      "Epoch 4/20 Batch 16/68 loss 0.30494531989097595\n",
      "Epoch 4/20 Batch 17/68 loss 0.22282131016254425\n",
      "Epoch 4/20 Batch 18/68 loss 0.30797460675239563\n",
      "Epoch 4/20 Batch 19/68 loss 0.22643998265266418\n",
      "Epoch 4/20 Batch 20/68 loss 0.3834908604621887\n",
      "Epoch 4/20 Batch 21/68 loss 0.2872425317764282\n",
      "Epoch 4/20 Batch 22/68 loss 0.21709147095680237\n",
      "Epoch 4/20 Batch 23/68 loss 0.1708904206752777\n",
      "Epoch 4/20 Batch 24/68 loss 0.1846807301044464\n",
      "Epoch 4/20 Batch 25/68 loss 0.45111849904060364\n",
      "Epoch 4/20 Batch 26/68 loss 0.21822816133499146\n",
      "Epoch 4/20 Batch 27/68 loss 0.3979593813419342\n",
      "Epoch 4/20 Batch 28/68 loss 0.1120227798819542\n",
      "Epoch 4/20 Batch 29/68 loss 0.20350301265716553\n",
      "Epoch 4/20 Batch 30/68 loss 0.19585345685482025\n",
      "Epoch 4/20 Batch 31/68 loss 0.4298299252986908\n",
      "Epoch 4/20 Batch 32/68 loss 0.14274927973747253\n",
      "Epoch 4/20 Batch 33/68 loss 0.24011647701263428\n",
      "Epoch 4/20 Batch 34/68 loss 0.1414717435836792\n",
      "Epoch 4/20 Batch 35/68 loss 0.18171238899230957\n",
      "Epoch 4/20 Batch 36/68 loss 0.2130340039730072\n",
      "Epoch 4/20 Batch 37/68 loss 0.1466168910264969\n",
      "Epoch 4/20 Batch 38/68 loss 0.12435422837734222\n",
      "Epoch 4/20 Batch 39/68 loss 0.2987757921218872\n",
      "Epoch 4/20 Batch 40/68 loss 0.12236522138118744\n",
      "Epoch 4/20 Batch 41/68 loss 0.3061926066875458\n",
      "Epoch 4/20 Batch 42/68 loss 0.21412920951843262\n",
      "Epoch 4/20 Batch 43/68 loss 0.1200893223285675\n",
      "Epoch 4/20 Batch 44/68 loss 0.2924146056175232\n",
      "Epoch 4/20 Batch 45/68 loss 0.43108657002449036\n",
      "Epoch 4/20 Batch 46/68 loss 0.10740512609481812\n",
      "Epoch 4/20 Batch 47/68 loss 0.09498263150453568\n",
      "Epoch 4/20 Batch 48/68 loss 0.07238923758268356\n",
      "Epoch 4/20 Batch 49/68 loss 0.08250746130943298\n",
      "Epoch 4/20 Batch 50/68 loss 0.1747366487979889\n",
      "Epoch 4/20 Batch 51/68 loss 0.20752879977226257\n",
      "Epoch 4/20 Batch 52/68 loss 0.1961018294095993\n",
      "Epoch 4/20 Batch 53/68 loss 0.223234161734581\n",
      "Epoch 4/20 Batch 54/68 loss 0.21485620737075806\n",
      "Epoch 4/20 Batch 55/68 loss 0.11761786043643951\n",
      "Epoch 4/20 Batch 56/68 loss 0.2946586310863495\n",
      "Epoch 4/20 Batch 57/68 loss 0.22423823177814484\n",
      "Epoch 4/20 Batch 58/68 loss 0.13091468811035156\n",
      "Epoch 4/20 Batch 59/68 loss 0.24359914660453796\n",
      "Epoch 4/20 Batch 60/68 loss 0.1837625652551651\n",
      "Epoch 4/20 Batch 61/68 loss 0.1595255583524704\n",
      "Epoch 4/20 Batch 62/68 loss 0.15748991072177887\n",
      "Epoch 4/20 Batch 63/68 loss 0.10665420442819595\n",
      "Epoch 4/20 Batch 64/68 loss 0.15944956243038177\n",
      "Epoch 4/20 Batch 65/68 loss 0.4632698595523834\n",
      "Epoch 4/20 Batch 66/68 loss 0.29437562823295593\n",
      "Epoch 4/20 Batch 67/68 loss 0.09845459461212158\n",
      "Epoch 4/20 Batch 68/68 loss 0.03603946790099144\n",
      "Epoch 5/20 Batch 1/68 loss 0.25596219301223755\n",
      "Epoch 5/20 Batch 2/68 loss 0.14477328956127167\n",
      "Epoch 5/20 Batch 3/68 loss 0.10842184722423553\n",
      "Epoch 5/20 Batch 4/68 loss 0.07696659117937088\n",
      "Epoch 5/20 Batch 5/68 loss 0.12413431704044342\n",
      "Epoch 5/20 Batch 6/68 loss 0.13385379314422607\n",
      "Epoch 5/20 Batch 7/68 loss 0.44381219148635864\n",
      "Epoch 5/20 Batch 8/68 loss 0.3046523630619049\n",
      "Epoch 5/20 Batch 9/68 loss 0.07461082935333252\n",
      "Epoch 5/20 Batch 10/68 loss 0.12382873892784119\n",
      "Epoch 5/20 Batch 11/68 loss 0.1778726577758789\n",
      "Epoch 5/20 Batch 12/68 loss 0.1272023767232895\n",
      "Epoch 5/20 Batch 13/68 loss 0.10356062650680542\n",
      "Epoch 5/20 Batch 14/68 loss 0.10182605683803558\n",
      "Epoch 5/20 Batch 15/68 loss 0.06569834053516388\n",
      "Epoch 5/20 Batch 16/68 loss 0.19978457689285278\n",
      "Epoch 5/20 Batch 17/68 loss 0.1434766948223114\n",
      "Epoch 5/20 Batch 18/68 loss 0.11532147228717804\n",
      "Epoch 5/20 Batch 19/68 loss 0.14083874225616455\n",
      "Epoch 5/20 Batch 20/68 loss 0.12847881019115448\n",
      "Epoch 5/20 Batch 21/68 loss 0.23189686238765717\n",
      "Epoch 5/20 Batch 22/68 loss 0.2038610875606537\n",
      "Epoch 5/20 Batch 23/68 loss 0.2237010896205902\n",
      "Epoch 5/20 Batch 24/68 loss 0.11556969583034515\n",
      "Epoch 5/20 Batch 25/68 loss 0.2592163383960724\n",
      "Epoch 5/20 Batch 26/68 loss 0.11686082184314728\n",
      "Epoch 5/20 Batch 27/68 loss 0.09760457277297974\n",
      "Epoch 5/20 Batch 28/68 loss 0.14855968952178955\n",
      "Epoch 5/20 Batch 29/68 loss 0.18072113394737244\n",
      "Epoch 5/20 Batch 30/68 loss 0.16401724517345428\n",
      "Epoch 5/20 Batch 31/68 loss 0.07705265283584595\n",
      "Epoch 5/20 Batch 32/68 loss 0.11129303276538849\n",
      "Epoch 5/20 Batch 33/68 loss 0.08477505296468735\n",
      "Epoch 5/20 Batch 34/68 loss 0.24664701521396637\n",
      "Epoch 5/20 Batch 35/68 loss 0.1688634157180786\n",
      "Epoch 5/20 Batch 36/68 loss 0.05954580754041672\n",
      "Epoch 5/20 Batch 37/68 loss 0.08893219381570816\n",
      "Epoch 5/20 Batch 38/68 loss 0.06349103152751923\n",
      "Epoch 5/20 Batch 39/68 loss 0.18597975373268127\n",
      "Epoch 5/20 Batch 40/68 loss 0.2551847994327545\n",
      "Epoch 5/20 Batch 41/68 loss 0.1537579894065857\n",
      "Epoch 5/20 Batch 42/68 loss 0.08496789634227753\n",
      "Epoch 5/20 Batch 43/68 loss 0.06290607154369354\n",
      "Epoch 5/20 Batch 44/68 loss 0.11772274971008301\n",
      "Epoch 5/20 Batch 45/68 loss 0.07152076065540314\n",
      "Epoch 5/20 Batch 46/68 loss 0.1351051777601242\n",
      "Epoch 5/20 Batch 47/68 loss 0.09095918387174606\n",
      "Epoch 5/20 Batch 48/68 loss 0.14750033617019653\n",
      "Epoch 5/20 Batch 49/68 loss 0.4480012059211731\n",
      "Epoch 5/20 Batch 50/68 loss 0.11765378713607788\n",
      "Epoch 5/20 Batch 51/68 loss 0.057050660252571106\n",
      "Epoch 5/20 Batch 52/68 loss 0.1988946944475174\n",
      "Epoch 5/20 Batch 53/68 loss 0.14219960570335388\n",
      "Epoch 5/20 Batch 54/68 loss 0.12986727058887482\n",
      "Epoch 5/20 Batch 55/68 loss 0.1395547091960907\n",
      "Epoch 5/20 Batch 56/68 loss 0.30587807297706604\n",
      "Epoch 5/20 Batch 57/68 loss 0.09072248637676239\n",
      "Epoch 5/20 Batch 58/68 loss 0.07429253309965134\n",
      "Epoch 5/20 Batch 59/68 loss 0.40541329979896545\n",
      "Epoch 5/20 Batch 60/68 loss 0.1849582940340042\n",
      "Epoch 5/20 Batch 61/68 loss 0.16104622185230255\n",
      "Epoch 5/20 Batch 62/68 loss 0.21250982582569122\n",
      "Epoch 5/20 Batch 63/68 loss 0.20412833988666534\n",
      "Epoch 5/20 Batch 64/68 loss 0.09383775293827057\n",
      "Epoch 5/20 Batch 65/68 loss 0.050459206104278564\n",
      "Epoch 5/20 Batch 66/68 loss 0.0731508657336235\n",
      "Epoch 5/20 Batch 67/68 loss 0.03413648530840874\n",
      "Epoch 5/20 Batch 68/68 loss 0.06653023511171341\n",
      "Epoch 6/20 Batch 1/68 loss 0.1061563491821289\n",
      "Epoch 6/20 Batch 2/68 loss 0.10110798478126526\n",
      "Epoch 6/20 Batch 3/68 loss 0.11006728559732437\n",
      "Epoch 6/20 Batch 4/68 loss 0.06858355551958084\n",
      "Epoch 6/20 Batch 5/68 loss 0.05349547788500786\n",
      "Epoch 6/20 Batch 6/68 loss 0.07912473380565643\n",
      "Epoch 6/20 Batch 7/68 loss 0.08011140674352646\n",
      "Epoch 6/20 Batch 8/68 loss 0.12136566638946533\n",
      "Epoch 6/20 Batch 9/68 loss 0.11966175585985184\n",
      "Epoch 6/20 Batch 10/68 loss 0.08323237299919128\n",
      "Epoch 6/20 Batch 11/68 loss 0.0701628029346466\n",
      "Epoch 6/20 Batch 12/68 loss 0.08861632645130157\n",
      "Epoch 6/20 Batch 13/68 loss 0.29196110367774963\n",
      "Epoch 6/20 Batch 14/68 loss 0.05383480712771416\n",
      "Epoch 6/20 Batch 15/68 loss 0.08413746953010559\n",
      "Epoch 6/20 Batch 16/68 loss 0.1248466894030571\n",
      "Epoch 6/20 Batch 17/68 loss 0.0994037464261055\n",
      "Epoch 6/20 Batch 18/68 loss 0.09545499086380005\n",
      "Epoch 6/20 Batch 19/68 loss 0.1284606009721756\n",
      "Epoch 6/20 Batch 20/68 loss 0.07857535779476166\n",
      "Epoch 6/20 Batch 21/68 loss 0.2756252884864807\n",
      "Epoch 6/20 Batch 22/68 loss 0.18022805452346802\n",
      "Epoch 6/20 Batch 23/68 loss 0.23533128201961517\n",
      "Epoch 6/20 Batch 24/68 loss 0.1515415757894516\n",
      "Epoch 6/20 Batch 25/68 loss 0.0704902708530426\n",
      "Epoch 6/20 Batch 26/68 loss 0.10456094145774841\n",
      "Epoch 6/20 Batch 27/68 loss 0.059758927673101425\n",
      "Epoch 6/20 Batch 28/68 loss 0.3032115399837494\n",
      "Epoch 6/20 Batch 29/68 loss 0.0467449426651001\n",
      "Epoch 6/20 Batch 30/68 loss 0.17601768672466278\n",
      "Epoch 6/20 Batch 31/68 loss 0.15525390207767487\n",
      "Epoch 6/20 Batch 32/68 loss 0.08081826567649841\n",
      "Epoch 6/20 Batch 33/68 loss 0.23607459664344788\n",
      "Epoch 6/20 Batch 34/68 loss 0.0646476000547409\n",
      "Epoch 6/20 Batch 35/68 loss 0.06380508840084076\n",
      "Epoch 6/20 Batch 36/68 loss 0.0686253160238266\n",
      "Epoch 6/20 Batch 37/68 loss 0.1504509299993515\n",
      "Epoch 6/20 Batch 38/68 loss 0.12985484302043915\n",
      "Epoch 6/20 Batch 39/68 loss 0.22754573822021484\n",
      "Epoch 6/20 Batch 40/68 loss 0.09212025254964828\n",
      "Epoch 6/20 Batch 41/68 loss 0.07111556828022003\n",
      "Epoch 6/20 Batch 42/68 loss 0.06420362740755081\n",
      "Epoch 6/20 Batch 43/68 loss 0.09728095680475235\n",
      "Epoch 6/20 Batch 44/68 loss 0.08189556002616882\n",
      "Epoch 6/20 Batch 45/68 loss 0.1065206453204155\n",
      "Epoch 6/20 Batch 46/68 loss 0.05032248795032501\n",
      "Epoch 6/20 Batch 47/68 loss 0.0743451863527298\n",
      "Epoch 6/20 Batch 48/68 loss 0.1529385894536972\n",
      "Epoch 6/20 Batch 49/68 loss 0.20356816053390503\n",
      "Epoch 6/20 Batch 50/68 loss 0.15742695331573486\n",
      "Epoch 6/20 Batch 51/68 loss 0.09944997727870941\n",
      "Epoch 6/20 Batch 52/68 loss 0.07707091420888901\n",
      "Epoch 6/20 Batch 53/68 loss 0.06748224049806595\n",
      "Epoch 6/20 Batch 54/68 loss 0.12034255266189575\n",
      "Epoch 6/20 Batch 55/68 loss 0.060568466782569885\n",
      "Epoch 6/20 Batch 56/68 loss 0.30064666271209717\n",
      "Epoch 6/20 Batch 57/68 loss 0.060582321137189865\n",
      "Epoch 6/20 Batch 58/68 loss 0.08198460936546326\n",
      "Epoch 6/20 Batch 59/68 loss 0.15359680354595184\n",
      "Epoch 6/20 Batch 60/68 loss 0.16134440898895264\n",
      "Epoch 6/20 Batch 61/68 loss 0.11984744668006897\n",
      "Epoch 6/20 Batch 62/68 loss 0.03250240907073021\n",
      "Epoch 6/20 Batch 63/68 loss 0.02527652680873871\n",
      "Epoch 6/20 Batch 64/68 loss 0.21599923074245453\n",
      "Epoch 6/20 Batch 65/68 loss 0.1124141663312912\n",
      "Epoch 6/20 Batch 66/68 loss 0.12349102646112442\n",
      "Epoch 6/20 Batch 67/68 loss 0.22199615836143494\n",
      "Epoch 6/20 Batch 68/68 loss 0.06644078344106674\n",
      "Epoch 7/20 Batch 1/68 loss 0.11155383288860321\n",
      "Epoch 7/20 Batch 2/68 loss 0.28192010521888733\n",
      "Epoch 7/20 Batch 3/68 loss 0.19895492494106293\n",
      "Epoch 7/20 Batch 4/68 loss 0.040506474673748016\n",
      "Epoch 7/20 Batch 5/68 loss 0.06694856286048889\n",
      "Epoch 7/20 Batch 6/68 loss 0.09271308034658432\n",
      "Epoch 7/20 Batch 7/68 loss 0.044047579169273376\n",
      "Epoch 7/20 Batch 8/68 loss 0.1277870386838913\n",
      "Epoch 7/20 Batch 9/68 loss 0.2764201760292053\n",
      "Epoch 7/20 Batch 10/68 loss 0.052971888333559036\n",
      "Epoch 7/20 Batch 11/68 loss 0.04338309168815613\n",
      "Epoch 7/20 Batch 12/68 loss 0.060418106615543365\n",
      "Epoch 7/20 Batch 13/68 loss 0.05436566472053528\n",
      "Epoch 7/20 Batch 14/68 loss 0.11673042923212051\n",
      "Epoch 7/20 Batch 15/68 loss 0.1408056765794754\n",
      "Epoch 7/20 Batch 16/68 loss 0.12322162091732025\n",
      "Epoch 7/20 Batch 17/68 loss 0.02966221421957016\n",
      "Epoch 7/20 Batch 18/68 loss 0.06775256991386414\n",
      "Epoch 7/20 Batch 19/68 loss 0.1612967550754547\n",
      "Epoch 7/20 Batch 20/68 loss 0.11387766897678375\n",
      "Epoch 7/20 Batch 21/68 loss 0.0760764330625534\n",
      "Epoch 7/20 Batch 22/68 loss 0.036287859082221985\n",
      "Epoch 7/20 Batch 23/68 loss 0.03783736005425453\n",
      "Epoch 7/20 Batch 24/68 loss 0.03763262927532196\n",
      "Epoch 7/20 Batch 25/68 loss 0.18632544577121735\n",
      "Epoch 7/20 Batch 26/68 loss 0.21590879559516907\n",
      "Epoch 7/20 Batch 27/68 loss 0.1755644977092743\n",
      "Epoch 7/20 Batch 28/68 loss 0.014051487669348717\n",
      "Epoch 7/20 Batch 29/68 loss 0.10794565081596375\n",
      "Epoch 7/20 Batch 30/68 loss 0.01958463340997696\n",
      "Epoch 7/20 Batch 31/68 loss 0.04668200761079788\n",
      "Epoch 7/20 Batch 32/68 loss 0.11117140203714371\n",
      "Epoch 7/20 Batch 33/68 loss 0.04653696343302727\n",
      "Epoch 7/20 Batch 34/68 loss 0.06535612791776657\n",
      "Epoch 7/20 Batch 35/68 loss 0.07066221535205841\n",
      "Epoch 7/20 Batch 36/68 loss 0.07071460783481598\n",
      "Epoch 7/20 Batch 37/68 loss 0.06265893578529358\n",
      "Epoch 7/20 Batch 38/68 loss 0.05342496186494827\n",
      "Epoch 7/20 Batch 39/68 loss 0.026297276839613914\n",
      "Epoch 7/20 Batch 40/68 loss 0.05880874767899513\n",
      "Epoch 7/20 Batch 41/68 loss 0.07998788356781006\n",
      "Epoch 7/20 Batch 42/68 loss 0.10072389245033264\n",
      "Epoch 7/20 Batch 43/68 loss 0.17210695147514343\n",
      "Epoch 7/20 Batch 44/68 loss 0.07235424220561981\n",
      "Epoch 7/20 Batch 45/68 loss 0.08441196382045746\n",
      "Epoch 7/20 Batch 46/68 loss 0.04888348653912544\n",
      "Epoch 7/20 Batch 47/68 loss 0.05625111609697342\n",
      "Epoch 7/20 Batch 48/68 loss 0.05702737718820572\n",
      "Epoch 7/20 Batch 49/68 loss 0.07278084754943848\n",
      "Epoch 7/20 Batch 50/68 loss 0.13271628320217133\n",
      "Epoch 7/20 Batch 51/68 loss 0.02658398449420929\n",
      "Epoch 7/20 Batch 52/68 loss 0.022540461272001266\n",
      "Epoch 7/20 Batch 53/68 loss 0.18633899092674255\n",
      "Epoch 7/20 Batch 54/68 loss 0.07971597462892532\n",
      "Epoch 7/20 Batch 55/68 loss 0.05772674083709717\n",
      "Epoch 7/20 Batch 56/68 loss 0.12850600481033325\n",
      "Epoch 7/20 Batch 57/68 loss 0.11829137802124023\n",
      "Epoch 7/20 Batch 58/68 loss 0.022703979164361954\n",
      "Epoch 7/20 Batch 59/68 loss 0.031601741909980774\n",
      "Epoch 7/20 Batch 60/68 loss 0.2830013334751129\n",
      "Epoch 7/20 Batch 61/68 loss 0.28517183661460876\n",
      "Epoch 7/20 Batch 62/68 loss 0.08532062917947769\n",
      "Epoch 7/20 Batch 63/68 loss 0.10540948063135147\n",
      "Epoch 7/20 Batch 64/68 loss 0.14074371755123138\n",
      "Epoch 7/20 Batch 65/68 loss 0.07055473327636719\n",
      "Epoch 7/20 Batch 66/68 loss 0.20741209387779236\n",
      "Epoch 7/20 Batch 67/68 loss 0.09820324927568436\n",
      "Epoch 7/20 Batch 68/68 loss 0.3058277666568756\n",
      "Epoch 8/20 Batch 1/68 loss 0.179930180311203\n",
      "Epoch 8/20 Batch 2/68 loss 0.061006203293800354\n",
      "Epoch 8/20 Batch 3/68 loss 0.02768179588019848\n",
      "Epoch 8/20 Batch 4/68 loss 0.04154527932405472\n",
      "Epoch 8/20 Batch 5/68 loss 0.15158003568649292\n",
      "Epoch 8/20 Batch 6/68 loss 0.06435970216989517\n",
      "Epoch 8/20 Batch 7/68 loss 0.13162969052791595\n",
      "Epoch 8/20 Batch 8/68 loss 0.04059112071990967\n",
      "Epoch 8/20 Batch 9/68 loss 0.2604734003543854\n",
      "Epoch 8/20 Batch 10/68 loss 0.03374817967414856\n",
      "Epoch 8/20 Batch 11/68 loss 0.16326919198036194\n",
      "Epoch 8/20 Batch 12/68 loss 0.0399152897298336\n",
      "Epoch 8/20 Batch 13/68 loss 0.08326192945241928\n",
      "Epoch 8/20 Batch 14/68 loss 0.05910610780119896\n",
      "Epoch 8/20 Batch 15/68 loss 0.05082430690526962\n",
      "Epoch 8/20 Batch 16/68 loss 0.03464417904615402\n",
      "Epoch 8/20 Batch 17/68 loss 0.23657657206058502\n",
      "Epoch 8/20 Batch 18/68 loss 0.08482271432876587\n",
      "Epoch 8/20 Batch 19/68 loss 0.17093278467655182\n",
      "Epoch 8/20 Batch 20/68 loss 0.12137411534786224\n",
      "Epoch 8/20 Batch 21/68 loss 0.08958829194307327\n",
      "Epoch 8/20 Batch 22/68 loss 0.09938031435012817\n",
      "Epoch 8/20 Batch 23/68 loss 0.03642493858933449\n",
      "Epoch 8/20 Batch 24/68 loss 0.07007963210344315\n",
      "Epoch 8/20 Batch 25/68 loss 0.09762366116046906\n",
      "Epoch 8/20 Batch 26/68 loss 0.14021936058998108\n",
      "Epoch 8/20 Batch 27/68 loss 0.22907860577106476\n",
      "Epoch 8/20 Batch 28/68 loss 0.051935140043497086\n",
      "Epoch 8/20 Batch 29/68 loss 0.07215362787246704\n",
      "Epoch 8/20 Batch 30/68 loss 0.11852815747261047\n",
      "Epoch 8/20 Batch 31/68 loss 0.01274008210748434\n",
      "Epoch 8/20 Batch 32/68 loss 0.0550885871052742\n",
      "Epoch 8/20 Batch 33/68 loss 0.05920742452144623\n",
      "Epoch 8/20 Batch 34/68 loss 0.05380639806389809\n",
      "Epoch 8/20 Batch 35/68 loss 0.05896011367440224\n",
      "Epoch 8/20 Batch 36/68 loss 0.04286082834005356\n",
      "Epoch 8/20 Batch 37/68 loss 0.22548502683639526\n",
      "Epoch 8/20 Batch 38/68 loss 0.056601956486701965\n",
      "Epoch 8/20 Batch 39/68 loss 0.029331786558032036\n",
      "Epoch 8/20 Batch 40/68 loss 0.1432795524597168\n",
      "Epoch 8/20 Batch 41/68 loss 0.06106672063469887\n",
      "Epoch 8/20 Batch 42/68 loss 0.1559237837791443\n",
      "Epoch 8/20 Batch 43/68 loss 0.1728474348783493\n",
      "Epoch 8/20 Batch 44/68 loss 0.09535246342420578\n",
      "Epoch 8/20 Batch 45/68 loss 0.05509713664650917\n",
      "Epoch 8/20 Batch 46/68 loss 0.1083495169878006\n",
      "Epoch 8/20 Batch 47/68 loss 0.028658142313361168\n",
      "Epoch 8/20 Batch 48/68 loss 0.06423310935497284\n",
      "Epoch 8/20 Batch 49/68 loss 0.09749646484851837\n",
      "Epoch 8/20 Batch 50/68 loss 0.11171033978462219\n",
      "Epoch 8/20 Batch 51/68 loss 0.10853197425603867\n",
      "Epoch 8/20 Batch 52/68 loss 0.04135683551430702\n",
      "Epoch 8/20 Batch 53/68 loss 0.07491400837898254\n",
      "Epoch 8/20 Batch 54/68 loss 0.05464673414826393\n",
      "Epoch 8/20 Batch 55/68 loss 0.040892988443374634\n",
      "Epoch 8/20 Batch 56/68 loss 0.053529635071754456\n",
      "Epoch 8/20 Batch 57/68 loss 0.07385004311800003\n",
      "Epoch 8/20 Batch 58/68 loss 0.019718457013368607\n",
      "Epoch 8/20 Batch 59/68 loss 0.09078317135572433\n",
      "Epoch 8/20 Batch 60/68 loss 0.04778174310922623\n",
      "Epoch 8/20 Batch 61/68 loss 0.02710261195898056\n",
      "Epoch 8/20 Batch 62/68 loss 0.012273004278540611\n",
      "Epoch 8/20 Batch 63/68 loss 0.03882863372564316\n",
      "Epoch 8/20 Batch 64/68 loss 0.32229748368263245\n",
      "Epoch 8/20 Batch 65/68 loss 0.09992175549268723\n",
      "Epoch 8/20 Batch 66/68 loss 0.037492536008358\n",
      "Epoch 8/20 Batch 67/68 loss 0.06762415170669556\n",
      "Epoch 8/20 Batch 68/68 loss 0.040873486548662186\n",
      "Epoch 9/20 Batch 1/68 loss 0.3165472149848938\n",
      "Epoch 9/20 Batch 2/68 loss 0.025450466200709343\n",
      "Epoch 9/20 Batch 3/68 loss 0.07429748773574829\n",
      "Epoch 9/20 Batch 4/68 loss 0.05641801655292511\n",
      "Epoch 9/20 Batch 5/68 loss 0.0718550831079483\n",
      "Epoch 9/20 Batch 6/68 loss 0.09652092307806015\n",
      "Epoch 9/20 Batch 7/68 loss 0.059897180646657944\n",
      "Epoch 9/20 Batch 8/68 loss 0.07064158469438553\n",
      "Epoch 9/20 Batch 9/68 loss 0.08649495244026184\n",
      "Epoch 9/20 Batch 10/68 loss 0.11702926456928253\n",
      "Epoch 9/20 Batch 11/68 loss 0.11657378822565079\n",
      "Epoch 9/20 Batch 12/68 loss 0.04206772521138191\n",
      "Epoch 9/20 Batch 13/68 loss 0.09320258349180222\n",
      "Epoch 9/20 Batch 14/68 loss 0.07195103168487549\n",
      "Epoch 9/20 Batch 15/68 loss 0.046455807983875275\n",
      "Epoch 9/20 Batch 16/68 loss 0.04558583348989487\n",
      "Epoch 9/20 Batch 17/68 loss 0.12141624093055725\n",
      "Epoch 9/20 Batch 18/68 loss 0.08297193050384521\n",
      "Epoch 9/20 Batch 19/68 loss 0.0269639752805233\n",
      "Epoch 9/20 Batch 20/68 loss 0.02470046654343605\n",
      "Epoch 9/20 Batch 21/68 loss 0.09326212108135223\n",
      "Epoch 9/20 Batch 22/68 loss 0.01938924379646778\n",
      "Epoch 9/20 Batch 23/68 loss 0.031572841107845306\n",
      "Epoch 9/20 Batch 24/68 loss 0.0547727532684803\n",
      "Epoch 9/20 Batch 25/68 loss 0.07684983313083649\n",
      "Epoch 9/20 Batch 26/68 loss 0.09546535462141037\n",
      "Epoch 9/20 Batch 27/68 loss 0.10379171371459961\n",
      "Epoch 9/20 Batch 28/68 loss 0.031125839799642563\n",
      "Epoch 9/20 Batch 29/68 loss 0.04131663218140602\n",
      "Epoch 9/20 Batch 30/68 loss 0.045161910355091095\n",
      "Epoch 9/20 Batch 31/68 loss 0.06243851035833359\n",
      "Epoch 9/20 Batch 32/68 loss 0.05903279408812523\n",
      "Epoch 9/20 Batch 33/68 loss 0.0448819175362587\n",
      "Epoch 9/20 Batch 34/68 loss 0.05999677628278732\n",
      "Epoch 9/20 Batch 35/68 loss 0.041009798645973206\n",
      "Epoch 9/20 Batch 36/68 loss 0.14473025500774384\n",
      "Epoch 9/20 Batch 37/68 loss 0.053563930094242096\n",
      "Epoch 9/20 Batch 38/68 loss 0.05934372544288635\n",
      "Epoch 9/20 Batch 39/68 loss 0.12967051565647125\n",
      "Epoch 9/20 Batch 40/68 loss 0.02771580033004284\n",
      "Epoch 9/20 Batch 41/68 loss 0.023117851465940475\n",
      "Epoch 9/20 Batch 42/68 loss 0.03486204892396927\n",
      "Epoch 9/20 Batch 43/68 loss 0.14735646545886993\n",
      "Epoch 9/20 Batch 44/68 loss 0.07286277413368225\n",
      "Epoch 9/20 Batch 45/68 loss 0.19434058666229248\n",
      "Epoch 9/20 Batch 46/68 loss 0.02766456827521324\n",
      "Epoch 9/20 Batch 47/68 loss 0.2812991440296173\n",
      "Epoch 9/20 Batch 48/68 loss 0.037834130227565765\n",
      "Epoch 9/20 Batch 49/68 loss 0.03531477227807045\n",
      "Epoch 9/20 Batch 50/68 loss 0.04813925176858902\n",
      "Epoch 9/20 Batch 51/68 loss 0.04612874612212181\n",
      "Epoch 9/20 Batch 52/68 loss 0.048653095960617065\n",
      "Epoch 9/20 Batch 53/68 loss 0.05287783220410347\n",
      "Epoch 9/20 Batch 54/68 loss 0.06708430498838425\n",
      "Epoch 9/20 Batch 55/68 loss 0.05033988505601883\n",
      "Epoch 9/20 Batch 56/68 loss 0.00938885472714901\n",
      "Epoch 9/20 Batch 57/68 loss 0.038412101566791534\n",
      "Epoch 9/20 Batch 58/68 loss 0.017046291381120682\n",
      "Epoch 9/20 Batch 59/68 loss 0.20754560828208923\n",
      "Epoch 9/20 Batch 60/68 loss 0.04041604697704315\n",
      "Epoch 9/20 Batch 61/68 loss 0.03087284043431282\n",
      "Epoch 9/20 Batch 62/68 loss 0.12826071679592133\n",
      "Epoch 9/20 Batch 63/68 loss 0.013653368689119816\n",
      "Epoch 9/20 Batch 64/68 loss 0.11127828061580658\n",
      "Epoch 9/20 Batch 65/68 loss 0.03363712504506111\n",
      "Epoch 9/20 Batch 66/68 loss 0.08492334187030792\n",
      "Epoch 9/20 Batch 67/68 loss 0.04574604704976082\n",
      "Epoch 9/20 Batch 68/68 loss 0.010885373689234257\n",
      "Epoch 10/20 Batch 1/68 loss 0.029930520802736282\n",
      "Epoch 10/20 Batch 2/68 loss 0.0282414760440588\n",
      "Epoch 10/20 Batch 3/68 loss 0.1168920248746872\n",
      "Epoch 10/20 Batch 4/68 loss 0.056762926280498505\n",
      "Epoch 10/20 Batch 5/68 loss 0.14016278088092804\n",
      "Epoch 10/20 Batch 6/68 loss 0.018484773114323616\n",
      "Epoch 10/20 Batch 7/68 loss 0.08627952635288239\n",
      "Epoch 10/20 Batch 8/68 loss 0.021798409521579742\n",
      "Epoch 10/20 Batch 9/68 loss 0.0448504313826561\n",
      "Epoch 10/20 Batch 10/68 loss 0.020718595013022423\n",
      "Epoch 10/20 Batch 11/68 loss 0.05740003660321236\n",
      "Epoch 10/20 Batch 12/68 loss 0.20216010510921478\n",
      "Epoch 10/20 Batch 13/68 loss 0.03736162185668945\n",
      "Epoch 10/20 Batch 14/68 loss 0.03342167288064957\n",
      "Epoch 10/20 Batch 15/68 loss 0.02991480752825737\n",
      "Epoch 10/20 Batch 16/68 loss 0.04765940457582474\n",
      "Epoch 10/20 Batch 17/68 loss 0.0646565854549408\n",
      "Epoch 10/20 Batch 18/68 loss 0.04250171035528183\n",
      "Epoch 10/20 Batch 19/68 loss 0.03571746498346329\n",
      "Epoch 10/20 Batch 20/68 loss 0.02668294683098793\n",
      "Epoch 10/20 Batch 21/68 loss 0.04260661453008652\n",
      "Epoch 10/20 Batch 22/68 loss 0.06489110738039017\n",
      "Epoch 10/20 Batch 23/68 loss 0.021317243576049805\n",
      "Epoch 10/20 Batch 24/68 loss 0.007907267659902573\n",
      "Epoch 10/20 Batch 25/68 loss 0.045675814151763916\n",
      "Epoch 10/20 Batch 26/68 loss 0.06761983782052994\n",
      "Epoch 10/20 Batch 27/68 loss 0.014300402253866196\n",
      "Epoch 10/20 Batch 28/68 loss 0.08571972697973251\n",
      "Epoch 10/20 Batch 29/68 loss 0.10997652262449265\n",
      "Epoch 10/20 Batch 30/68 loss 0.18035465478897095\n",
      "Epoch 10/20 Batch 31/68 loss 0.13726186752319336\n",
      "Epoch 10/20 Batch 32/68 loss 0.027435749769210815\n",
      "Epoch 10/20 Batch 33/68 loss 0.06456160545349121\n",
      "Epoch 10/20 Batch 34/68 loss 0.23892691731452942\n",
      "Epoch 10/20 Batch 35/68 loss 0.035966068506240845\n",
      "Epoch 10/20 Batch 36/68 loss 0.024634113535284996\n",
      "Epoch 10/20 Batch 37/68 loss 0.07458195090293884\n",
      "Epoch 10/20 Batch 38/68 loss 0.053567323833703995\n",
      "Epoch 10/20 Batch 39/68 loss 0.036369435489177704\n",
      "Epoch 10/20 Batch 40/68 loss 0.035822756588459015\n",
      "Epoch 10/20 Batch 41/68 loss 0.010757286101579666\n",
      "Epoch 10/20 Batch 42/68 loss 0.08732660859823227\n",
      "Epoch 10/20 Batch 43/68 loss 0.06485903263092041\n",
      "Epoch 10/20 Batch 44/68 loss 0.014066829346120358\n",
      "Epoch 10/20 Batch 45/68 loss 0.05251752957701683\n",
      "Epoch 10/20 Batch 46/68 loss 0.06630025058984756\n",
      "Epoch 10/20 Batch 47/68 loss 0.02746608853340149\n",
      "Epoch 10/20 Batch 48/68 loss 0.026917390525341034\n",
      "Epoch 10/20 Batch 49/68 loss 0.09375534951686859\n",
      "Epoch 10/20 Batch 50/68 loss 0.15728574991226196\n",
      "Epoch 10/20 Batch 51/68 loss 0.014520182274281979\n",
      "Epoch 10/20 Batch 52/68 loss 0.0861125960946083\n",
      "Epoch 10/20 Batch 53/68 loss 0.10262028872966766\n",
      "Epoch 10/20 Batch 54/68 loss 0.11476922780275345\n",
      "Epoch 10/20 Batch 55/68 loss 0.034472644329071045\n",
      "Epoch 10/20 Batch 56/68 loss 0.022304914891719818\n",
      "Epoch 10/20 Batch 57/68 loss 0.04031938314437866\n",
      "Epoch 10/20 Batch 58/68 loss 0.06449577212333679\n",
      "Epoch 10/20 Batch 59/68 loss 0.04668387398123741\n",
      "Epoch 10/20 Batch 60/68 loss 0.045176152139902115\n",
      "Epoch 10/20 Batch 61/68 loss 0.05245412141084671\n",
      "Epoch 10/20 Batch 62/68 loss 0.04614622890949249\n",
      "Epoch 10/20 Batch 63/68 loss 0.025051172822713852\n",
      "Epoch 10/20 Batch 64/68 loss 0.11071569472551346\n",
      "Epoch 10/20 Batch 65/68 loss 0.2732125520706177\n",
      "Epoch 10/20 Batch 66/68 loss 0.02106410264968872\n",
      "Epoch 10/20 Batch 67/68 loss 0.14332890510559082\n",
      "Epoch 10/20 Batch 68/68 loss 0.10699344426393509\n",
      "Epoch 11/20 Batch 1/68 loss 0.036157190799713135\n",
      "Epoch 11/20 Batch 2/68 loss 0.01957731693983078\n",
      "Epoch 11/20 Batch 3/68 loss 0.20905227959156036\n",
      "Epoch 11/20 Batch 4/68 loss 0.09182454645633698\n",
      "Epoch 11/20 Batch 5/68 loss 0.05530724301934242\n",
      "Epoch 11/20 Batch 6/68 loss 0.037898290902376175\n",
      "Epoch 11/20 Batch 7/68 loss 0.03294571116566658\n",
      "Epoch 11/20 Batch 8/68 loss 0.03164966404438019\n",
      "Epoch 11/20 Batch 9/68 loss 0.02135402336716652\n",
      "Epoch 11/20 Batch 10/68 loss 0.061323996633291245\n",
      "Epoch 11/20 Batch 11/68 loss 0.04240349307656288\n",
      "Epoch 11/20 Batch 12/68 loss 0.02801218256354332\n",
      "Epoch 11/20 Batch 13/68 loss 0.04542248323559761\n",
      "Epoch 11/20 Batch 14/68 loss 0.06344206631183624\n",
      "Epoch 11/20 Batch 15/68 loss 0.018457187339663506\n",
      "Epoch 11/20 Batch 16/68 loss 0.027245689183473587\n",
      "Epoch 11/20 Batch 17/68 loss 0.14167115092277527\n",
      "Epoch 11/20 Batch 18/68 loss 0.054728396236896515\n",
      "Epoch 11/20 Batch 19/68 loss 0.021318860352039337\n",
      "Epoch 11/20 Batch 20/68 loss 0.02273228019475937\n",
      "Epoch 11/20 Batch 21/68 loss 0.2525694668292999\n",
      "Epoch 11/20 Batch 22/68 loss 0.04894661530852318\n",
      "Epoch 11/20 Batch 23/68 loss 0.12622509896755219\n",
      "Epoch 11/20 Batch 24/68 loss 0.06793072819709778\n",
      "Epoch 11/20 Batch 25/68 loss 0.03047797828912735\n",
      "Epoch 11/20 Batch 26/68 loss 0.012942686676979065\n",
      "Epoch 11/20 Batch 27/68 loss 0.06077411770820618\n",
      "Epoch 11/20 Batch 28/68 loss 0.08985673636198044\n",
      "Epoch 11/20 Batch 29/68 loss 0.05170564353466034\n",
      "Epoch 11/20 Batch 30/68 loss 0.037298835813999176\n",
      "Epoch 11/20 Batch 31/68 loss 0.014392423443496227\n",
      "Epoch 11/20 Batch 32/68 loss 0.016293616965413094\n",
      "Epoch 11/20 Batch 33/68 loss 0.04948057606816292\n",
      "Epoch 11/20 Batch 34/68 loss 0.022570349276065826\n",
      "Epoch 11/20 Batch 35/68 loss 0.011855296790599823\n",
      "Epoch 11/20 Batch 36/68 loss 0.04851127043366432\n",
      "Epoch 11/20 Batch 37/68 loss 0.05309736356139183\n",
      "Epoch 11/20 Batch 38/68 loss 0.01760261505842209\n",
      "Epoch 11/20 Batch 39/68 loss 0.08370065689086914\n",
      "Epoch 11/20 Batch 40/68 loss 0.00986984372138977\n",
      "Epoch 11/20 Batch 41/68 loss 0.03232907876372337\n",
      "Epoch 11/20 Batch 42/68 loss 0.013940333388745785\n",
      "Epoch 11/20 Batch 43/68 loss 0.20298878848552704\n",
      "Epoch 11/20 Batch 44/68 loss 0.023549867793917656\n",
      "Epoch 11/20 Batch 45/68 loss 0.013834873214364052\n",
      "Epoch 11/20 Batch 46/68 loss 0.047634683549404144\n",
      "Epoch 11/20 Batch 47/68 loss 0.16721083223819733\n",
      "Epoch 11/20 Batch 48/68 loss 0.06800588220357895\n",
      "Epoch 11/20 Batch 49/68 loss 0.041914526373147964\n",
      "Epoch 11/20 Batch 50/68 loss 0.10317451506853104\n",
      "Epoch 11/20 Batch 51/68 loss 0.07144001871347427\n",
      "Epoch 11/20 Batch 52/68 loss 0.01803884655237198\n",
      "Epoch 11/20 Batch 53/68 loss 0.05511923506855965\n",
      "Epoch 11/20 Batch 54/68 loss 0.028157152235507965\n",
      "Epoch 11/20 Batch 55/68 loss 0.114395871758461\n",
      "Epoch 11/20 Batch 56/68 loss 0.05043886974453926\n",
      "Epoch 11/20 Batch 57/68 loss 0.01665285788476467\n",
      "Epoch 11/20 Batch 58/68 loss 0.04477805644273758\n",
      "Epoch 11/20 Batch 59/68 loss 0.0341603085398674\n",
      "Epoch 11/20 Batch 60/68 loss 0.03110838308930397\n",
      "Epoch 11/20 Batch 61/68 loss 0.056318920105695724\n",
      "Epoch 11/20 Batch 62/68 loss 0.06623947620391846\n",
      "Epoch 11/20 Batch 63/68 loss 0.04105759039521217\n",
      "Epoch 11/20 Batch 64/68 loss 0.013426436111330986\n",
      "Epoch 11/20 Batch 65/68 loss 0.04336109757423401\n",
      "Epoch 11/20 Batch 66/68 loss 0.09726843237876892\n",
      "Epoch 11/20 Batch 67/68 loss 0.021166983991861343\n",
      "Epoch 11/20 Batch 68/68 loss 0.04988974332809448\n",
      "Epoch 12/20 Batch 1/68 loss 0.054591089487075806\n",
      "Epoch 12/20 Batch 2/68 loss 0.02209615334868431\n",
      "Epoch 12/20 Batch 3/68 loss 0.034917764365673065\n",
      "Epoch 12/20 Batch 4/68 loss 0.033334676176309586\n",
      "Epoch 12/20 Batch 5/68 loss 0.023799767717719078\n",
      "Epoch 12/20 Batch 6/68 loss 0.025882462039589882\n",
      "Epoch 12/20 Batch 7/68 loss 0.05462498217821121\n",
      "Epoch 12/20 Batch 8/68 loss 0.015782399103045464\n",
      "Epoch 12/20 Batch 9/68 loss 0.04683782160282135\n",
      "Epoch 12/20 Batch 10/68 loss 0.02446046844124794\n",
      "Epoch 12/20 Batch 11/68 loss 0.05992240086197853\n",
      "Epoch 12/20 Batch 12/68 loss 0.03069460764527321\n",
      "Epoch 12/20 Batch 13/68 loss 0.08486327528953552\n",
      "Epoch 12/20 Batch 14/68 loss 0.08235052973031998\n",
      "Epoch 12/20 Batch 15/68 loss 0.017007552087306976\n",
      "Epoch 12/20 Batch 16/68 loss 0.01225870568305254\n",
      "Epoch 12/20 Batch 17/68 loss 0.05154115706682205\n",
      "Epoch 12/20 Batch 18/68 loss 0.18566583096981049\n",
      "Epoch 12/20 Batch 19/68 loss 0.030626695603132248\n",
      "Epoch 12/20 Batch 20/68 loss 0.0200458113104105\n",
      "Epoch 12/20 Batch 21/68 loss 0.015298265963792801\n",
      "Epoch 12/20 Batch 22/68 loss 0.07008416950702667\n",
      "Epoch 12/20 Batch 23/68 loss 0.011659912765026093\n",
      "Epoch 12/20 Batch 24/68 loss 0.06250619143247604\n",
      "Epoch 12/20 Batch 25/68 loss 0.020685026422142982\n",
      "Epoch 12/20 Batch 26/68 loss 0.06757916510105133\n",
      "Epoch 12/20 Batch 27/68 loss 0.011907591484487057\n",
      "Epoch 12/20 Batch 28/68 loss 0.014097296632826328\n",
      "Epoch 12/20 Batch 29/68 loss 0.1813376098871231\n",
      "Epoch 12/20 Batch 30/68 loss 0.18248873949050903\n",
      "Epoch 12/20 Batch 31/68 loss 0.02593645267188549\n",
      "Epoch 12/20 Batch 32/68 loss 0.04555012658238411\n",
      "Epoch 12/20 Batch 33/68 loss 0.01632380299270153\n",
      "Epoch 12/20 Batch 34/68 loss 0.024172935634851456\n",
      "Epoch 12/20 Batch 35/68 loss 0.03425627201795578\n",
      "Epoch 12/20 Batch 36/68 loss 0.021603547036647797\n",
      "Epoch 12/20 Batch 37/68 loss 0.20099548995494843\n",
      "Epoch 12/20 Batch 38/68 loss 0.022472813725471497\n",
      "Epoch 12/20 Batch 39/68 loss 0.06742920726537704\n",
      "Epoch 12/20 Batch 40/68 loss 0.035318516194820404\n",
      "Epoch 12/20 Batch 41/68 loss 0.018299348652362823\n",
      "Epoch 12/20 Batch 42/68 loss 0.04434809461236\n",
      "Epoch 12/20 Batch 43/68 loss 0.13409554958343506\n",
      "Epoch 12/20 Batch 44/68 loss 0.06416454911231995\n",
      "Epoch 12/20 Batch 45/68 loss 0.11196964979171753\n",
      "Epoch 12/20 Batch 46/68 loss 0.042609669268131256\n",
      "Epoch 12/20 Batch 47/68 loss 0.041698284447193146\n",
      "Epoch 12/20 Batch 48/68 loss 0.02466929703950882\n",
      "Epoch 12/20 Batch 49/68 loss 0.04022109508514404\n",
      "Epoch 12/20 Batch 50/68 loss 0.09890690445899963\n",
      "Epoch 12/20 Batch 51/68 loss 0.0075874668546020985\n",
      "Epoch 12/20 Batch 52/68 loss 0.0272078700363636\n",
      "Epoch 12/20 Batch 53/68 loss 0.047692105174064636\n",
      "Epoch 12/20 Batch 54/68 loss 0.028885241597890854\n",
      "Epoch 12/20 Batch 55/68 loss 0.0342702716588974\n",
      "Epoch 12/20 Batch 56/68 loss 0.061177752912044525\n",
      "Epoch 12/20 Batch 57/68 loss 0.024170756340026855\n",
      "Epoch 12/20 Batch 58/68 loss 0.03720290958881378\n",
      "Epoch 12/20 Batch 59/68 loss 0.03331587091088295\n",
      "Epoch 12/20 Batch 60/68 loss 0.02077140472829342\n",
      "Epoch 12/20 Batch 61/68 loss 0.04005264863371849\n",
      "Epoch 12/20 Batch 62/68 loss 0.08269765973091125\n",
      "Epoch 12/20 Batch 63/68 loss 0.08357609063386917\n",
      "Epoch 12/20 Batch 64/68 loss 0.012876057997345924\n",
      "Epoch 12/20 Batch 65/68 loss 0.02436218038201332\n",
      "Epoch 12/20 Batch 66/68 loss 0.027153367176651955\n",
      "Epoch 12/20 Batch 67/68 loss 0.06267949193716049\n",
      "Epoch 12/20 Batch 68/68 loss 0.024101510643959045\n",
      "Epoch 13/20 Batch 1/68 loss 0.05129483714699745\n",
      "Epoch 13/20 Batch 2/68 loss 0.022587692365050316\n",
      "Epoch 13/20 Batch 3/68 loss 0.03444889560341835\n",
      "Epoch 13/20 Batch 4/68 loss 0.019499557092785835\n",
      "Epoch 13/20 Batch 5/68 loss 0.06949838995933533\n",
      "Epoch 13/20 Batch 6/68 loss 0.01988213323056698\n",
      "Epoch 13/20 Batch 7/68 loss 0.039625946432352066\n",
      "Epoch 13/20 Batch 8/68 loss 0.07063429057598114\n",
      "Epoch 13/20 Batch 9/68 loss 0.06666526943445206\n",
      "Epoch 13/20 Batch 10/68 loss 0.04914792254567146\n",
      "Epoch 13/20 Batch 11/68 loss 0.013163881376385689\n",
      "Epoch 13/20 Batch 12/68 loss 0.07372603565454483\n",
      "Epoch 13/20 Batch 13/68 loss 0.05213797092437744\n",
      "Epoch 13/20 Batch 14/68 loss 0.030001739040017128\n",
      "Epoch 13/20 Batch 15/68 loss 0.04157380014657974\n",
      "Epoch 13/20 Batch 16/68 loss 0.02121102064847946\n",
      "Epoch 13/20 Batch 17/68 loss 0.1515309363603592\n",
      "Epoch 13/20 Batch 18/68 loss 0.11581096053123474\n",
      "Epoch 13/20 Batch 19/68 loss 0.04186349734663963\n",
      "Epoch 13/20 Batch 20/68 loss 0.07524676620960236\n",
      "Epoch 13/20 Batch 21/68 loss 0.06856495141983032\n",
      "Epoch 13/20 Batch 22/68 loss 0.032404590398073196\n",
      "Epoch 13/20 Batch 23/68 loss 0.012512207962572575\n",
      "Epoch 13/20 Batch 24/68 loss 0.008916963823139668\n",
      "Epoch 13/20 Batch 25/68 loss 0.03783707320690155\n",
      "Epoch 13/20 Batch 26/68 loss 0.03941207006573677\n",
      "Epoch 13/20 Batch 27/68 loss 0.009692080318927765\n",
      "Epoch 13/20 Batch 28/68 loss 0.030651293694972992\n",
      "Epoch 13/20 Batch 29/68 loss 0.04457678645849228\n",
      "Epoch 13/20 Batch 30/68 loss 0.027010159566998482\n",
      "Epoch 13/20 Batch 31/68 loss 0.013857902027666569\n",
      "Epoch 13/20 Batch 32/68 loss 0.022784490138292313\n",
      "Epoch 13/20 Batch 33/68 loss 0.11468813568353653\n",
      "Epoch 13/20 Batch 34/68 loss 0.0320473276078701\n",
      "Epoch 13/20 Batch 35/68 loss 0.02046225406229496\n",
      "Epoch 13/20 Batch 36/68 loss 0.025333918631076813\n",
      "Epoch 13/20 Batch 37/68 loss 0.038739703595638275\n",
      "Epoch 13/20 Batch 38/68 loss 0.0759943276643753\n",
      "Epoch 13/20 Batch 39/68 loss 0.02563559263944626\n",
      "Epoch 13/20 Batch 40/68 loss 0.07139132916927338\n",
      "Epoch 13/20 Batch 41/68 loss 0.025139767676591873\n",
      "Epoch 13/20 Batch 42/68 loss 0.05120459198951721\n",
      "Epoch 13/20 Batch 43/68 loss 0.020264258608222008\n",
      "Epoch 13/20 Batch 44/68 loss 0.01980512961745262\n",
      "Epoch 13/20 Batch 45/68 loss 0.03653251379728317\n",
      "Epoch 13/20 Batch 46/68 loss 0.004078218247741461\n",
      "Epoch 13/20 Batch 47/68 loss 0.024462100118398666\n",
      "Epoch 13/20 Batch 48/68 loss 0.07456067949533463\n",
      "Epoch 13/20 Batch 49/68 loss 0.04503876343369484\n",
      "Epoch 13/20 Batch 50/68 loss 0.05471128970384598\n",
      "Epoch 13/20 Batch 51/68 loss 0.013808413408696651\n",
      "Epoch 13/20 Batch 52/68 loss 0.13602891564369202\n",
      "Epoch 13/20 Batch 53/68 loss 0.008998965844511986\n",
      "Epoch 13/20 Batch 54/68 loss 0.009697669185698032\n",
      "Epoch 13/20 Batch 55/68 loss 0.006825151853263378\n",
      "Epoch 13/20 Batch 56/68 loss 0.10265262424945831\n",
      "Epoch 13/20 Batch 57/68 loss 0.2037917524576187\n",
      "Epoch 13/20 Batch 58/68 loss 0.05149025470018387\n",
      "Epoch 13/20 Batch 59/68 loss 0.015498080290853977\n",
      "Epoch 13/20 Batch 60/68 loss 0.03718864545226097\n",
      "Epoch 13/20 Batch 61/68 loss 0.00867991428822279\n",
      "Epoch 13/20 Batch 62/68 loss 0.010194860398769379\n",
      "Epoch 13/20 Batch 63/68 loss 0.07066705077886581\n",
      "Epoch 13/20 Batch 64/68 loss 0.02133117988705635\n",
      "Epoch 13/20 Batch 65/68 loss 0.009411180391907692\n",
      "Epoch 13/20 Batch 66/68 loss 0.0180965606123209\n",
      "Epoch 13/20 Batch 67/68 loss 0.009849930182099342\n",
      "Epoch 13/20 Batch 68/68 loss 0.0054824925027787685\n",
      "Epoch 14/20 Batch 1/68 loss 0.04896732047200203\n",
      "Epoch 14/20 Batch 2/68 loss 0.060131095349788666\n",
      "Epoch 14/20 Batch 3/68 loss 0.09092225134372711\n",
      "Epoch 14/20 Batch 4/68 loss 0.09452509135007858\n",
      "Epoch 14/20 Batch 5/68 loss 0.02082892879843712\n",
      "Epoch 14/20 Batch 6/68 loss 0.13232716917991638\n",
      "Epoch 14/20 Batch 7/68 loss 0.07632917910814285\n",
      "Epoch 14/20 Batch 8/68 loss 0.012695569545030594\n",
      "Epoch 14/20 Batch 9/68 loss 0.054326243698596954\n",
      "Epoch 14/20 Batch 10/68 loss 0.01680576056241989\n",
      "Epoch 14/20 Batch 11/68 loss 0.022796859964728355\n",
      "Epoch 14/20 Batch 12/68 loss 0.015958670526742935\n",
      "Epoch 14/20 Batch 13/68 loss 0.023425955325365067\n",
      "Epoch 14/20 Batch 14/68 loss 0.03880855813622475\n",
      "Epoch 14/20 Batch 15/68 loss 0.024405166506767273\n",
      "Epoch 14/20 Batch 16/68 loss 0.04584456607699394\n",
      "Epoch 14/20 Batch 17/68 loss 0.006271430756896734\n",
      "Epoch 14/20 Batch 18/68 loss 0.00807416532188654\n",
      "Epoch 14/20 Batch 19/68 loss 0.047005027532577515\n",
      "Epoch 14/20 Batch 20/68 loss 0.04079703986644745\n",
      "Epoch 14/20 Batch 21/68 loss 0.013688757084310055\n",
      "Epoch 14/20 Batch 22/68 loss 0.00597852049395442\n",
      "Epoch 14/20 Batch 23/68 loss 0.011497607454657555\n",
      "Epoch 14/20 Batch 24/68 loss 0.008070331066846848\n",
      "Epoch 14/20 Batch 25/68 loss 0.05967193469405174\n",
      "Epoch 14/20 Batch 26/68 loss 0.02145771123468876\n",
      "Epoch 14/20 Batch 27/68 loss 0.024786919355392456\n",
      "Epoch 14/20 Batch 28/68 loss 0.023711003363132477\n",
      "Epoch 14/20 Batch 29/68 loss 0.10078414529561996\n",
      "Epoch 14/20 Batch 30/68 loss 0.05617355927824974\n",
      "Epoch 14/20 Batch 31/68 loss 0.009060194715857506\n",
      "Epoch 14/20 Batch 32/68 loss 0.0555221363902092\n",
      "Epoch 14/20 Batch 33/68 loss 0.040719546377658844\n",
      "Epoch 14/20 Batch 34/68 loss 0.014438128098845482\n",
      "Epoch 14/20 Batch 35/68 loss 0.01336002629250288\n",
      "Epoch 14/20 Batch 36/68 loss 0.032960452139377594\n",
      "Epoch 14/20 Batch 37/68 loss 0.14291571080684662\n",
      "Epoch 14/20 Batch 38/68 loss 0.026772914454340935\n",
      "Epoch 14/20 Batch 39/68 loss 0.01694951392710209\n",
      "Epoch 14/20 Batch 40/68 loss 0.05820637568831444\n",
      "Epoch 14/20 Batch 41/68 loss 0.025978000834584236\n",
      "Epoch 14/20 Batch 42/68 loss 0.00945619959384203\n",
      "Epoch 14/20 Batch 43/68 loss 0.01584990881383419\n",
      "Epoch 14/20 Batch 44/68 loss 0.055140890181064606\n",
      "Epoch 14/20 Batch 45/68 loss 0.011231903918087482\n",
      "Epoch 14/20 Batch 46/68 loss 0.024799996986985207\n",
      "Epoch 14/20 Batch 47/68 loss 0.048144418746232986\n",
      "Epoch 14/20 Batch 48/68 loss 0.013777552172541618\n",
      "Epoch 14/20 Batch 49/68 loss 0.007073330692946911\n",
      "Epoch 14/20 Batch 50/68 loss 0.04138609394431114\n",
      "Epoch 14/20 Batch 51/68 loss 0.026127438992261887\n",
      "Epoch 14/20 Batch 52/68 loss 0.04565870761871338\n",
      "Epoch 14/20 Batch 53/68 loss 0.07319095730781555\n",
      "Epoch 14/20 Batch 54/68 loss 0.008237883448600769\n",
      "Epoch 14/20 Batch 55/68 loss 0.1695849746465683\n",
      "Epoch 14/20 Batch 56/68 loss 0.02024819329380989\n",
      "Epoch 14/20 Batch 57/68 loss 0.038844335824251175\n",
      "Epoch 14/20 Batch 58/68 loss 0.0716385692358017\n",
      "Epoch 14/20 Batch 59/68 loss 0.09155178815126419\n",
      "Epoch 14/20 Batch 60/68 loss 0.06867571175098419\n",
      "Epoch 14/20 Batch 61/68 loss 0.021856607869267464\n",
      "Epoch 14/20 Batch 62/68 loss 0.03356732428073883\n",
      "Epoch 14/20 Batch 63/68 loss 0.01563773676753044\n",
      "Epoch 14/20 Batch 64/68 loss 0.08521965891122818\n",
      "Epoch 14/20 Batch 65/68 loss 0.026145294308662415\n",
      "Epoch 14/20 Batch 66/68 loss 0.012157592922449112\n",
      "Epoch 14/20 Batch 67/68 loss 0.034775134176015854\n",
      "Epoch 14/20 Batch 68/68 loss 0.015509071759879589\n",
      "Epoch 15/20 Batch 1/68 loss 0.03894798457622528\n",
      "Epoch 15/20 Batch 2/68 loss 0.04783996194601059\n",
      "Epoch 15/20 Batch 3/68 loss 0.040192633867263794\n",
      "Epoch 15/20 Batch 4/68 loss 0.03917136788368225\n",
      "Epoch 15/20 Batch 5/68 loss 0.02588474005460739\n",
      "Epoch 15/20 Batch 6/68 loss 0.050507981330156326\n",
      "Epoch 15/20 Batch 7/68 loss 0.020575664937496185\n",
      "Epoch 15/20 Batch 8/68 loss 0.04600167274475098\n",
      "Epoch 15/20 Batch 9/68 loss 0.02107156813144684\n",
      "Epoch 15/20 Batch 10/68 loss 0.03992603346705437\n",
      "Epoch 15/20 Batch 11/68 loss 0.020113999024033546\n",
      "Epoch 15/20 Batch 12/68 loss 0.022060953080654144\n",
      "Epoch 15/20 Batch 13/68 loss 0.010322670452296734\n",
      "Epoch 15/20 Batch 14/68 loss 0.015368356369435787\n",
      "Epoch 15/20 Batch 15/68 loss 0.0245729461312294\n",
      "Epoch 15/20 Batch 16/68 loss 0.033327627927064896\n",
      "Epoch 15/20 Batch 17/68 loss 0.04551373049616814\n",
      "Epoch 15/20 Batch 18/68 loss 0.0025124577805399895\n",
      "Epoch 15/20 Batch 19/68 loss 0.030242621898651123\n",
      "Epoch 15/20 Batch 20/68 loss 0.009167185053229332\n",
      "Epoch 15/20 Batch 21/68 loss 0.01328806672245264\n",
      "Epoch 15/20 Batch 22/68 loss 0.052609819918870926\n",
      "Epoch 15/20 Batch 23/68 loss 0.05173946171998978\n",
      "Epoch 15/20 Batch 24/68 loss 0.025659915059804916\n",
      "Epoch 15/20 Batch 25/68 loss 0.020530154928565025\n",
      "Epoch 15/20 Batch 26/68 loss 0.010161721147596836\n",
      "Epoch 15/20 Batch 27/68 loss 0.09622614085674286\n",
      "Epoch 15/20 Batch 28/68 loss 0.031415220350027084\n",
      "Epoch 15/20 Batch 29/68 loss 0.03483186662197113\n",
      "Epoch 15/20 Batch 30/68 loss 0.009636682458221912\n",
      "Epoch 15/20 Batch 31/68 loss 0.034855056554079056\n",
      "Epoch 15/20 Batch 32/68 loss 0.011806503869593143\n",
      "Epoch 15/20 Batch 33/68 loss 0.010431350208818913\n",
      "Epoch 15/20 Batch 34/68 loss 0.029173070564866066\n",
      "Epoch 15/20 Batch 35/68 loss 0.04857342317700386\n",
      "Epoch 15/20 Batch 36/68 loss 0.010337283834815025\n",
      "Epoch 15/20 Batch 37/68 loss 0.07946297526359558\n",
      "Epoch 15/20 Batch 38/68 loss 0.025078199803829193\n",
      "Epoch 15/20 Batch 39/68 loss 0.06698518246412277\n",
      "Epoch 15/20 Batch 40/68 loss 0.027323007583618164\n",
      "Epoch 15/20 Batch 41/68 loss 0.038013290613889694\n",
      "Epoch 15/20 Batch 42/68 loss 0.005513956770300865\n",
      "Epoch 15/20 Batch 43/68 loss 0.049730945378541946\n",
      "Epoch 15/20 Batch 44/68 loss 0.13135604560375214\n",
      "Epoch 15/20 Batch 45/68 loss 0.02188970521092415\n",
      "Epoch 15/20 Batch 46/68 loss 0.07812006771564484\n",
      "Epoch 15/20 Batch 47/68 loss 0.01839527115225792\n",
      "Epoch 15/20 Batch 48/68 loss 0.023924777284264565\n",
      "Epoch 15/20 Batch 49/68 loss 0.08415643870830536\n",
      "Epoch 15/20 Batch 50/68 loss 0.027214892208576202\n",
      "Epoch 15/20 Batch 51/68 loss 0.008569248020648956\n",
      "Epoch 15/20 Batch 52/68 loss 0.020900951698422432\n",
      "Epoch 15/20 Batch 53/68 loss 0.01622844859957695\n",
      "Epoch 15/20 Batch 54/68 loss 0.04026533290743828\n",
      "Epoch 15/20 Batch 55/68 loss 0.047179143875837326\n",
      "Epoch 15/20 Batch 56/68 loss 0.11962594091892242\n",
      "Epoch 15/20 Batch 57/68 loss 0.01061929389834404\n",
      "Epoch 15/20 Batch 58/68 loss 0.04244335740804672\n",
      "Epoch 15/20 Batch 59/68 loss 0.028433864936232567\n",
      "Epoch 15/20 Batch 60/68 loss 0.010778529569506645\n",
      "Epoch 15/20 Batch 61/68 loss 0.05956060811877251\n",
      "Epoch 15/20 Batch 62/68 loss 0.008082356303930283\n",
      "Epoch 15/20 Batch 63/68 loss 0.035170335322618484\n",
      "Epoch 15/20 Batch 64/68 loss 0.01874428614974022\n",
      "Epoch 15/20 Batch 65/68 loss 0.021945301443338394\n",
      "Epoch 15/20 Batch 66/68 loss 0.02100052684545517\n",
      "Epoch 15/20 Batch 67/68 loss 0.006224134936928749\n",
      "Epoch 15/20 Batch 68/68 loss 0.0011145019670948386\n",
      "Epoch 16/20 Batch 1/68 loss 0.09269317984580994\n",
      "Epoch 16/20 Batch 2/68 loss 0.024841323494911194\n",
      "Epoch 16/20 Batch 3/68 loss 0.0026215841062366962\n",
      "Epoch 16/20 Batch 4/68 loss 0.007745572365820408\n",
      "Epoch 16/20 Batch 5/68 loss 0.012675710953772068\n",
      "Epoch 16/20 Batch 6/68 loss 0.030240891501307487\n",
      "Epoch 16/20 Batch 7/68 loss 0.05868276581168175\n",
      "Epoch 16/20 Batch 8/68 loss 0.024740533903241158\n",
      "Epoch 16/20 Batch 9/68 loss 0.010024985298514366\n",
      "Epoch 16/20 Batch 10/68 loss 0.034482479095458984\n",
      "Epoch 16/20 Batch 11/68 loss 0.02046327292919159\n",
      "Epoch 16/20 Batch 12/68 loss 0.00937001220881939\n",
      "Epoch 16/20 Batch 13/68 loss 0.08497149497270584\n",
      "Epoch 16/20 Batch 14/68 loss 0.013828504830598831\n",
      "Epoch 16/20 Batch 15/68 loss 0.025408532470464706\n",
      "Epoch 16/20 Batch 16/68 loss 0.007970764301717281\n",
      "Epoch 16/20 Batch 17/68 loss 0.015037846751511097\n",
      "Epoch 16/20 Batch 18/68 loss 0.0226515494287014\n",
      "Epoch 16/20 Batch 19/68 loss 0.012396562844514847\n",
      "Epoch 16/20 Batch 20/68 loss 0.03206365928053856\n",
      "Epoch 16/20 Batch 21/68 loss 0.01326089445501566\n",
      "Epoch 16/20 Batch 22/68 loss 0.004831117112189531\n",
      "Epoch 16/20 Batch 23/68 loss 0.03890576213598251\n",
      "Epoch 16/20 Batch 24/68 loss 0.04660264030098915\n",
      "Epoch 16/20 Batch 25/68 loss 0.007907392457127571\n",
      "Epoch 16/20 Batch 26/68 loss 0.02892949804663658\n",
      "Epoch 16/20 Batch 27/68 loss 0.01935824379324913\n",
      "Epoch 16/20 Batch 28/68 loss 0.02502250112593174\n",
      "Epoch 16/20 Batch 29/68 loss 0.021660564467310905\n",
      "Epoch 16/20 Batch 30/68 loss 0.056897297501564026\n",
      "Epoch 16/20 Batch 31/68 loss 0.04487001523375511\n",
      "Epoch 16/20 Batch 32/68 loss 0.026936927810311317\n",
      "Epoch 16/20 Batch 33/68 loss 0.028814498335123062\n",
      "Epoch 16/20 Batch 34/68 loss 0.010256180539727211\n",
      "Epoch 16/20 Batch 35/68 loss 0.015084215439856052\n",
      "Epoch 16/20 Batch 36/68 loss 0.014150341972708702\n",
      "Epoch 16/20 Batch 37/68 loss 0.011780519038438797\n",
      "Epoch 16/20 Batch 38/68 loss 0.031136147677898407\n",
      "Epoch 16/20 Batch 39/68 loss 0.004571566358208656\n",
      "Epoch 16/20 Batch 40/68 loss 0.0381852425634861\n",
      "Epoch 16/20 Batch 41/68 loss 0.005825503263622522\n",
      "Epoch 16/20 Batch 42/68 loss 0.014708214439451694\n",
      "Epoch 16/20 Batch 43/68 loss 0.01505039818584919\n",
      "Epoch 16/20 Batch 44/68 loss 0.013187841512262821\n",
      "Epoch 16/20 Batch 45/68 loss 0.020259147509932518\n",
      "Epoch 16/20 Batch 46/68 loss 0.027812965214252472\n",
      "Epoch 16/20 Batch 47/68 loss 0.062091048806905746\n",
      "Epoch 16/20 Batch 48/68 loss 0.016364384442567825\n",
      "Epoch 16/20 Batch 49/68 loss 0.03447537124156952\n",
      "Epoch 16/20 Batch 50/68 loss 0.00741728488355875\n",
      "Epoch 16/20 Batch 51/68 loss 0.03489664942026138\n",
      "Epoch 16/20 Batch 52/68 loss 0.01714983768761158\n",
      "Epoch 16/20 Batch 53/68 loss 0.03666405752301216\n",
      "Epoch 16/20 Batch 54/68 loss 0.014074200764298439\n",
      "Epoch 16/20 Batch 55/68 loss 0.19612669944763184\n",
      "Epoch 16/20 Batch 56/68 loss 0.013081054203212261\n",
      "Epoch 16/20 Batch 57/68 loss 0.022078685462474823\n",
      "Epoch 16/20 Batch 58/68 loss 0.1000569760799408\n",
      "Epoch 16/20 Batch 59/68 loss 0.04635826498270035\n",
      "Epoch 16/20 Batch 60/68 loss 0.01905769109725952\n",
      "Epoch 16/20 Batch 61/68 loss 0.040289368480443954\n",
      "Epoch 16/20 Batch 62/68 loss 0.030785486102104187\n",
      "Epoch 16/20 Batch 63/68 loss 0.05677225813269615\n",
      "Epoch 16/20 Batch 64/68 loss 0.057383179664611816\n",
      "Epoch 16/20 Batch 65/68 loss 0.09239384531974792\n",
      "Epoch 16/20 Batch 66/68 loss 0.036687955260276794\n",
      "Epoch 16/20 Batch 67/68 loss 0.012061524204909801\n",
      "Epoch 16/20 Batch 68/68 loss 0.025766713544726372\n",
      "Epoch 17/20 Batch 1/68 loss 0.042782917618751526\n",
      "Epoch 17/20 Batch 2/68 loss 0.01910513825714588\n",
      "Epoch 17/20 Batch 3/68 loss 0.026304207742214203\n",
      "Epoch 17/20 Batch 4/68 loss 0.035112813115119934\n",
      "Epoch 17/20 Batch 5/68 loss 0.016663808375597\n",
      "Epoch 17/20 Batch 6/68 loss 0.011417122557759285\n",
      "Epoch 17/20 Batch 7/68 loss 0.037550751119852066\n",
      "Epoch 17/20 Batch 8/68 loss 0.08230935037136078\n",
      "Epoch 17/20 Batch 9/68 loss 0.018244566395878792\n",
      "Epoch 17/20 Batch 10/68 loss 0.011135587468743324\n",
      "Epoch 17/20 Batch 11/68 loss 0.0026738359592854977\n",
      "Epoch 17/20 Batch 12/68 loss 0.006830249913036823\n",
      "Epoch 17/20 Batch 13/68 loss 0.009691531769931316\n",
      "Epoch 17/20 Batch 14/68 loss 0.00774034671485424\n",
      "Epoch 17/20 Batch 15/68 loss 0.008216733112931252\n",
      "Epoch 17/20 Batch 16/68 loss 0.0074743470177054405\n",
      "Epoch 17/20 Batch 17/68 loss 0.01669628545641899\n",
      "Epoch 17/20 Batch 18/68 loss 0.02224913239479065\n",
      "Epoch 17/20 Batch 19/68 loss 0.01912660151720047\n",
      "Epoch 17/20 Batch 20/68 loss 0.012442018836736679\n",
      "Epoch 17/20 Batch 21/68 loss 0.019134698435664177\n",
      "Epoch 17/20 Batch 22/68 loss 0.0009352428023703396\n",
      "Epoch 17/20 Batch 23/68 loss 0.03337149694561958\n",
      "Epoch 17/20 Batch 24/68 loss 0.010392059572041035\n",
      "Epoch 17/20 Batch 25/68 loss 0.022443272173404694\n",
      "Epoch 17/20 Batch 26/68 loss 0.010317928157746792\n",
      "Epoch 17/20 Batch 27/68 loss 0.008524504490196705\n",
      "Epoch 17/20 Batch 28/68 loss 0.015066795982420444\n",
      "Epoch 17/20 Batch 29/68 loss 0.01105294656008482\n",
      "Epoch 17/20 Batch 30/68 loss 0.01746932975947857\n",
      "Epoch 17/20 Batch 31/68 loss 0.015245241113007069\n",
      "Epoch 17/20 Batch 32/68 loss 0.006038193590939045\n",
      "Epoch 17/20 Batch 33/68 loss 0.03392838314175606\n",
      "Epoch 17/20 Batch 34/68 loss 0.024691950529813766\n",
      "Epoch 17/20 Batch 35/68 loss 0.04309337958693504\n",
      "Epoch 17/20 Batch 36/68 loss 0.016377614811062813\n",
      "Epoch 17/20 Batch 37/68 loss 0.07801715284585953\n",
      "Epoch 17/20 Batch 38/68 loss 0.037443336099386215\n",
      "Epoch 17/20 Batch 39/68 loss 0.05773649737238884\n",
      "Epoch 17/20 Batch 40/68 loss 0.03848227858543396\n",
      "Epoch 17/20 Batch 41/68 loss 0.059000447392463684\n",
      "Epoch 17/20 Batch 42/68 loss 0.010642468929290771\n",
      "Epoch 17/20 Batch 43/68 loss 0.01326101552695036\n",
      "Epoch 17/20 Batch 44/68 loss 0.03595609590411186\n",
      "Epoch 17/20 Batch 45/68 loss 0.018915802240371704\n",
      "Epoch 17/20 Batch 46/68 loss 0.02222813107073307\n",
      "Epoch 17/20 Batch 47/68 loss 0.07266510277986526\n",
      "Epoch 17/20 Batch 48/68 loss 0.02870003879070282\n",
      "Epoch 17/20 Batch 49/68 loss 0.051070667803287506\n",
      "Epoch 17/20 Batch 50/68 loss 0.0166656244546175\n",
      "Epoch 17/20 Batch 51/68 loss 0.030211975798010826\n",
      "Epoch 17/20 Batch 52/68 loss 0.07199615240097046\n",
      "Epoch 17/20 Batch 53/68 loss 0.06298808008432388\n",
      "Epoch 17/20 Batch 54/68 loss 0.013080131262540817\n",
      "Epoch 17/20 Batch 55/68 loss 0.009063179604709148\n",
      "Epoch 17/20 Batch 56/68 loss 0.05927355960011482\n",
      "Epoch 17/20 Batch 57/68 loss 0.017614416778087616\n",
      "Epoch 17/20 Batch 58/68 loss 0.036425381898880005\n",
      "Epoch 17/20 Batch 59/68 loss 0.024788741022348404\n",
      "Epoch 17/20 Batch 60/68 loss 0.04264174401760101\n",
      "Epoch 17/20 Batch 61/68 loss 0.06264117360115051\n",
      "Epoch 17/20 Batch 62/68 loss 0.05787132680416107\n",
      "Epoch 17/20 Batch 63/68 loss 0.017697613686323166\n",
      "Epoch 17/20 Batch 64/68 loss 0.02527766488492489\n",
      "Epoch 17/20 Batch 65/68 loss 0.01584925688803196\n",
      "Epoch 17/20 Batch 66/68 loss 0.06094968318939209\n",
      "Epoch 17/20 Batch 67/68 loss 0.06217162311077118\n",
      "Epoch 17/20 Batch 68/68 loss 0.0014001174131408334\n",
      "Epoch 18/20 Batch 1/68 loss 0.08619378507137299\n",
      "Epoch 18/20 Batch 2/68 loss 0.00648688618093729\n",
      "Epoch 18/20 Batch 3/68 loss 0.005381443537771702\n",
      "Epoch 18/20 Batch 4/68 loss 0.008026317693293095\n",
      "Epoch 18/20 Batch 5/68 loss 0.022217387333512306\n",
      "Epoch 18/20 Batch 6/68 loss 0.013876059092581272\n",
      "Epoch 18/20 Batch 7/68 loss 0.05607188493013382\n",
      "Epoch 18/20 Batch 8/68 loss 0.01617281511425972\n",
      "Epoch 18/20 Batch 9/68 loss 0.002871890552341938\n",
      "Epoch 18/20 Batch 10/68 loss 0.008725742809474468\n",
      "Epoch 18/20 Batch 11/68 loss 0.024108530953526497\n",
      "Epoch 18/20 Batch 12/68 loss 0.013857876881957054\n",
      "Epoch 18/20 Batch 13/68 loss 0.004398693330585957\n",
      "Epoch 18/20 Batch 14/68 loss 0.026403333991765976\n",
      "Epoch 18/20 Batch 15/68 loss 0.011674663983285427\n",
      "Epoch 18/20 Batch 16/68 loss 0.026063984259963036\n",
      "Epoch 18/20 Batch 17/68 loss 0.04357270523905754\n",
      "Epoch 18/20 Batch 18/68 loss 0.01405125018209219\n",
      "Epoch 18/20 Batch 19/68 loss 0.02981557697057724\n",
      "Epoch 18/20 Batch 20/68 loss 0.13964194059371948\n",
      "Epoch 18/20 Batch 21/68 loss 0.00263233738951385\n",
      "Epoch 18/20 Batch 22/68 loss 0.04097194969654083\n",
      "Epoch 18/20 Batch 23/68 loss 0.04379367083311081\n",
      "Epoch 18/20 Batch 24/68 loss 0.016307726502418518\n",
      "Epoch 18/20 Batch 25/68 loss 0.03305415064096451\n",
      "Epoch 18/20 Batch 26/68 loss 0.007524481508880854\n",
      "Epoch 18/20 Batch 27/68 loss 0.004011699929833412\n",
      "Epoch 18/20 Batch 28/68 loss 0.040229085832834244\n",
      "Epoch 18/20 Batch 29/68 loss 0.014384145848453045\n",
      "Epoch 18/20 Batch 30/68 loss 0.007448763120919466\n",
      "Epoch 18/20 Batch 31/68 loss 0.014588899910449982\n",
      "Epoch 18/20 Batch 32/68 loss 0.00988680124282837\n",
      "Epoch 18/20 Batch 33/68 loss 0.013375155627727509\n",
      "Epoch 18/20 Batch 34/68 loss 0.012813909910619259\n",
      "Epoch 18/20 Batch 35/68 loss 0.07743646204471588\n",
      "Epoch 18/20 Batch 36/68 loss 0.004873862490057945\n",
      "Epoch 18/20 Batch 37/68 loss 0.09048858284950256\n",
      "Epoch 18/20 Batch 38/68 loss 0.01601702906191349\n",
      "Epoch 18/20 Batch 39/68 loss 0.007779266219586134\n",
      "Epoch 18/20 Batch 40/68 loss 0.02257619798183441\n",
      "Epoch 18/20 Batch 41/68 loss 0.02398420311510563\n",
      "Epoch 18/20 Batch 42/68 loss 0.03762764856219292\n",
      "Epoch 18/20 Batch 43/68 loss 0.011193275451660156\n",
      "Epoch 18/20 Batch 44/68 loss 0.05992971360683441\n",
      "Epoch 18/20 Batch 45/68 loss 0.041349031031131744\n",
      "Epoch 18/20 Batch 46/68 loss 0.008390593342483044\n",
      "Epoch 18/20 Batch 47/68 loss 0.01862495206296444\n",
      "Epoch 18/20 Batch 48/68 loss 0.01433867122977972\n",
      "Epoch 18/20 Batch 49/68 loss 0.010864007286727428\n",
      "Epoch 18/20 Batch 50/68 loss 0.00948357954621315\n",
      "Epoch 18/20 Batch 51/68 loss 0.014531196095049381\n",
      "Epoch 18/20 Batch 52/68 loss 0.020335804671049118\n",
      "Epoch 18/20 Batch 53/68 loss 0.021722454577684402\n",
      "Epoch 18/20 Batch 54/68 loss 0.09699664264917374\n",
      "Epoch 18/20 Batch 55/68 loss 0.04034891724586487\n",
      "Epoch 18/20 Batch 56/68 loss 0.03091978281736374\n",
      "Epoch 18/20 Batch 57/68 loss 0.03859515115618706\n",
      "Epoch 18/20 Batch 58/68 loss 0.053000278770923615\n",
      "Epoch 18/20 Batch 59/68 loss 0.004280484281480312\n",
      "Epoch 18/20 Batch 60/68 loss 0.024080898612737656\n",
      "Epoch 18/20 Batch 61/68 loss 0.0177929624915123\n",
      "Epoch 18/20 Batch 62/68 loss 0.007988776080310345\n",
      "Epoch 18/20 Batch 63/68 loss 0.011737649329006672\n",
      "Epoch 18/20 Batch 64/68 loss 0.011330705136060715\n",
      "Epoch 18/20 Batch 65/68 loss 0.01713644340634346\n",
      "Epoch 18/20 Batch 66/68 loss 0.01942629925906658\n",
      "Epoch 18/20 Batch 67/68 loss 0.03523440659046173\n",
      "Epoch 18/20 Batch 68/68 loss 0.026426712051033974\n",
      "Epoch 19/20 Batch 1/68 loss 0.0373990572988987\n",
      "Epoch 19/20 Batch 2/68 loss 0.0041968561708927155\n",
      "Epoch 19/20 Batch 3/68 loss 0.022699011489748955\n",
      "Epoch 19/20 Batch 4/68 loss 0.005169079173356295\n",
      "Epoch 19/20 Batch 5/68 loss 0.044001154601573944\n",
      "Epoch 19/20 Batch 6/68 loss 0.025454219430685043\n",
      "Epoch 19/20 Batch 7/68 loss 0.02356628328561783\n",
      "Epoch 19/20 Batch 8/68 loss 0.0958995521068573\n",
      "Epoch 19/20 Batch 9/68 loss 0.039002999663352966\n",
      "Epoch 19/20 Batch 10/68 loss 0.024151530116796494\n",
      "Epoch 19/20 Batch 11/68 loss 0.030966436490416527\n",
      "Epoch 19/20 Batch 12/68 loss 0.033617060631513596\n",
      "Epoch 19/20 Batch 13/68 loss 0.0018283352255821228\n",
      "Epoch 19/20 Batch 14/68 loss 0.0067411260679364204\n",
      "Epoch 19/20 Batch 15/68 loss 0.04194053262472153\n",
      "Epoch 19/20 Batch 16/68 loss 0.049076925963163376\n",
      "Epoch 19/20 Batch 17/68 loss 0.012425998225808144\n",
      "Epoch 19/20 Batch 18/68 loss 0.019471289590001106\n",
      "Epoch 19/20 Batch 19/68 loss 0.008953094482421875\n",
      "Epoch 19/20 Batch 20/68 loss 0.043459054082632065\n",
      "Epoch 19/20 Batch 21/68 loss 0.024507157504558563\n",
      "Epoch 19/20 Batch 22/68 loss 0.014261594973504543\n",
      "Epoch 19/20 Batch 23/68 loss 0.01409030333161354\n",
      "Epoch 19/20 Batch 24/68 loss 0.005980140995234251\n",
      "Epoch 19/20 Batch 25/68 loss 0.04767525941133499\n",
      "Epoch 19/20 Batch 26/68 loss 0.005183098372071981\n",
      "Epoch 19/20 Batch 27/68 loss 0.0125051811337471\n",
      "Epoch 19/20 Batch 28/68 loss 0.06870484352111816\n",
      "Epoch 19/20 Batch 29/68 loss 0.022570688277482986\n",
      "Epoch 19/20 Batch 30/68 loss 0.019648831337690353\n",
      "Epoch 19/20 Batch 31/68 loss 0.044350553303956985\n",
      "Epoch 19/20 Batch 32/68 loss 0.03791119530797005\n",
      "Epoch 19/20 Batch 33/68 loss 0.0015347355511039495\n",
      "Epoch 19/20 Batch 34/68 loss 0.012590424157679081\n",
      "Epoch 19/20 Batch 35/68 loss 0.0059168171137571335\n",
      "Epoch 19/20 Batch 36/68 loss 0.0281252171844244\n",
      "Epoch 19/20 Batch 37/68 loss 0.023511085659265518\n",
      "Epoch 19/20 Batch 38/68 loss 0.027039844542741776\n",
      "Epoch 19/20 Batch 39/68 loss 0.00786497350782156\n",
      "Epoch 19/20 Batch 40/68 loss 0.013255404308438301\n",
      "Epoch 19/20 Batch 41/68 loss 0.01342136412858963\n",
      "Epoch 19/20 Batch 42/68 loss 0.008905419148504734\n",
      "Epoch 19/20 Batch 43/68 loss 0.03225863352417946\n",
      "Epoch 19/20 Batch 44/68 loss 0.016227182000875473\n",
      "Epoch 19/20 Batch 45/68 loss 0.032625120133161545\n",
      "Epoch 19/20 Batch 46/68 loss 0.0064973607659339905\n",
      "Epoch 19/20 Batch 47/68 loss 0.010315201245248318\n",
      "Epoch 19/20 Batch 48/68 loss 0.00126513815484941\n",
      "Epoch 19/20 Batch 49/68 loss 0.02937021665275097\n",
      "Epoch 19/20 Batch 50/68 loss 0.1678471714258194\n",
      "Epoch 19/20 Batch 51/68 loss 0.02235456183552742\n",
      "Epoch 19/20 Batch 52/68 loss 0.01133680995553732\n",
      "Epoch 19/20 Batch 53/68 loss 0.0101432790979743\n",
      "Epoch 19/20 Batch 54/68 loss 0.00713972607627511\n",
      "Epoch 19/20 Batch 55/68 loss 0.02228092961013317\n",
      "Epoch 19/20 Batch 56/68 loss 0.012083033099770546\n",
      "Epoch 19/20 Batch 57/68 loss 0.009061782620847225\n",
      "Epoch 19/20 Batch 58/68 loss 0.03108043596148491\n",
      "Epoch 19/20 Batch 59/68 loss 0.018046695739030838\n",
      "Epoch 19/20 Batch 60/68 loss 0.026616785675287247\n",
      "Epoch 19/20 Batch 61/68 loss 0.007820901460945606\n",
      "Epoch 19/20 Batch 62/68 loss 0.022002968937158585\n",
      "Epoch 19/20 Batch 63/68 loss 0.01826646365225315\n",
      "Epoch 19/20 Batch 64/68 loss 0.02685372345149517\n",
      "Epoch 19/20 Batch 65/68 loss 0.08513440936803818\n",
      "Epoch 19/20 Batch 66/68 loss 0.022219588980078697\n",
      "Epoch 19/20 Batch 67/68 loss 0.07687985897064209\n",
      "Epoch 19/20 Batch 68/68 loss 0.007558189332485199\n",
      "Epoch 20/20 Batch 1/68 loss 0.0085503701120615\n",
      "Epoch 20/20 Batch 2/68 loss 0.013147596269845963\n",
      "Epoch 20/20 Batch 3/68 loss 0.01577088050544262\n",
      "Epoch 20/20 Batch 4/68 loss 0.005972974933683872\n",
      "Epoch 20/20 Batch 5/68 loss 0.011548633687198162\n",
      "Epoch 20/20 Batch 6/68 loss 0.011983228847384453\n",
      "Epoch 20/20 Batch 7/68 loss 0.07877654582262039\n",
      "Epoch 20/20 Batch 8/68 loss 0.019401853904128075\n",
      "Epoch 20/20 Batch 9/68 loss 0.010838746093213558\n",
      "Epoch 20/20 Batch 10/68 loss 0.017197832465171814\n",
      "Epoch 20/20 Batch 11/68 loss 0.005251601338386536\n",
      "Epoch 20/20 Batch 12/68 loss 0.010594016872346401\n",
      "Epoch 20/20 Batch 13/68 loss 0.012012776918709278\n",
      "Epoch 20/20 Batch 14/68 loss 0.03439933806657791\n",
      "Epoch 20/20 Batch 15/68 loss 0.020369838923215866\n",
      "Epoch 20/20 Batch 16/68 loss 0.012900174595415592\n",
      "Epoch 20/20 Batch 17/68 loss 0.009507968090474606\n",
      "Epoch 20/20 Batch 18/68 loss 0.00383498752489686\n",
      "Epoch 20/20 Batch 19/68 loss 0.023206407204270363\n",
      "Epoch 20/20 Batch 20/68 loss 0.01139067206531763\n",
      "Epoch 20/20 Batch 21/68 loss 0.002898881211876869\n",
      "Epoch 20/20 Batch 22/68 loss 0.08292786777019501\n",
      "Epoch 20/20 Batch 23/68 loss 0.007228667847812176\n",
      "Epoch 20/20 Batch 24/68 loss 0.01586923561990261\n",
      "Epoch 20/20 Batch 25/68 loss 0.01684233918786049\n",
      "Epoch 20/20 Batch 26/68 loss 0.008154276758432388\n",
      "Epoch 20/20 Batch 27/68 loss 0.0076066479086875916\n",
      "Epoch 20/20 Batch 28/68 loss 0.02033473365008831\n",
      "Epoch 20/20 Batch 29/68 loss 0.017184467986226082\n",
      "Epoch 20/20 Batch 30/68 loss 0.012963972054421902\n",
      "Epoch 20/20 Batch 31/68 loss 0.011989829130470753\n",
      "Epoch 20/20 Batch 32/68 loss 0.011863402090966702\n",
      "Epoch 20/20 Batch 33/68 loss 0.01281654741615057\n",
      "Epoch 20/20 Batch 34/68 loss 0.03271258622407913\n",
      "Epoch 20/20 Batch 35/68 loss 0.01235739141702652\n",
      "Epoch 20/20 Batch 36/68 loss 0.03162556514143944\n",
      "Epoch 20/20 Batch 37/68 loss 0.007666761055588722\n",
      "Epoch 20/20 Batch 38/68 loss 0.01545688509941101\n",
      "Epoch 20/20 Batch 39/68 loss 0.014439077116549015\n",
      "Epoch 20/20 Batch 40/68 loss 0.025945128872990608\n",
      "Epoch 20/20 Batch 41/68 loss 0.03901885822415352\n",
      "Epoch 20/20 Batch 42/68 loss 0.04693455621600151\n",
      "Epoch 20/20 Batch 43/68 loss 0.012435903772711754\n",
      "Epoch 20/20 Batch 44/68 loss 0.01996236853301525\n",
      "Epoch 20/20 Batch 45/68 loss 0.012465373612940311\n",
      "Epoch 20/20 Batch 46/68 loss 0.006681178696453571\n",
      "Epoch 20/20 Batch 47/68 loss 0.02188061736524105\n",
      "Epoch 20/20 Batch 48/68 loss 0.019594615325331688\n",
      "Epoch 20/20 Batch 49/68 loss 0.02959270402789116\n",
      "Epoch 20/20 Batch 50/68 loss 0.005469655618071556\n",
      "Epoch 20/20 Batch 51/68 loss 0.008373528718948364\n",
      "Epoch 20/20 Batch 52/68 loss 0.005372196901589632\n",
      "Epoch 20/20 Batch 53/68 loss 0.0027873176150023937\n",
      "Epoch 20/20 Batch 54/68 loss 0.03320596367120743\n",
      "Epoch 20/20 Batch 55/68 loss 0.007357715629041195\n",
      "Epoch 20/20 Batch 56/68 loss 0.030519619584083557\n",
      "Epoch 20/20 Batch 57/68 loss 0.030515404418110847\n",
      "Epoch 20/20 Batch 58/68 loss 0.04463648423552513\n",
      "Epoch 20/20 Batch 59/68 loss 0.05410078167915344\n",
      "Epoch 20/20 Batch 60/68 loss 0.03118508867919445\n",
      "Epoch 20/20 Batch 61/68 loss 0.01600179448723793\n",
      "Epoch 20/20 Batch 62/68 loss 0.014186428859829903\n",
      "Epoch 20/20 Batch 63/68 loss 0.05385517701506615\n",
      "Epoch 20/20 Batch 64/68 loss 0.006747659761458635\n",
      "Epoch 20/20 Batch 65/68 loss 0.011624762788414955\n",
      "Epoch 20/20 Batch 66/68 loss 0.04408848285675049\n",
      "Epoch 20/20 Batch 67/68 loss 0.004497221205383539\n",
      "Epoch 20/20 Batch 68/68 loss 0.01917468197643757\n",
      "Epoch 21/20 Batch 1/68 loss 0.0038678834680467844\n",
      "Epoch 21/20 Batch 2/68 loss 0.008088321425020695\n",
      "Epoch 21/20 Batch 3/68 loss 0.01479828730225563\n",
      "Epoch 21/20 Batch 4/68 loss 0.018650822341442108\n",
      "Epoch 21/20 Batch 5/68 loss 0.012290943413972855\n",
      "Epoch 21/20 Batch 6/68 loss 0.0053347814828157425\n",
      "Epoch 21/20 Batch 7/68 loss 0.019588293507695198\n",
      "Epoch 21/20 Batch 8/68 loss 0.007888306863605976\n",
      "Epoch 21/20 Batch 9/68 loss 0.00616734754294157\n",
      "Epoch 21/20 Batch 10/68 loss 0.014333376660943031\n",
      "Epoch 21/20 Batch 11/68 loss 0.014875045046210289\n",
      "Epoch 21/20 Batch 12/68 loss 0.023200061172246933\n",
      "Epoch 21/20 Batch 13/68 loss 0.015205185860395432\n",
      "Epoch 21/20 Batch 14/68 loss 0.04433673620223999\n",
      "Epoch 21/20 Batch 15/68 loss 0.01702602580189705\n",
      "Epoch 21/20 Batch 16/68 loss 0.008113796822726727\n",
      "Epoch 21/20 Batch 17/68 loss 0.018676701933145523\n",
      "Epoch 21/20 Batch 18/68 loss 0.017382685095071793\n",
      "Epoch 21/20 Batch 19/68 loss 0.03740964084863663\n",
      "Epoch 21/20 Batch 20/68 loss 0.003235196927562356\n",
      "Epoch 21/20 Batch 21/68 loss 0.010119597427546978\n",
      "Epoch 21/20 Batch 22/68 loss 0.014132803305983543\n",
      "Epoch 21/20 Batch 23/68 loss 0.027686789631843567\n",
      "Epoch 21/20 Batch 24/68 loss 0.011505262926220894\n",
      "Epoch 21/20 Batch 25/68 loss 0.04856586456298828\n",
      "Epoch 21/20 Batch 26/68 loss 0.022322550415992737\n",
      "Epoch 21/20 Batch 27/68 loss 0.0011916880030184984\n",
      "Epoch 21/20 Batch 28/68 loss 0.016940779983997345\n",
      "Epoch 21/20 Batch 29/68 loss 0.014607733115553856\n",
      "Epoch 21/20 Batch 30/68 loss 0.0022505116648972034\n",
      "Epoch 21/20 Batch 31/68 loss 0.03940474987030029\n",
      "Epoch 21/20 Batch 32/68 loss 0.006707282271236181\n",
      "Epoch 21/20 Batch 33/68 loss 0.03871419280767441\n",
      "Epoch 21/20 Batch 34/68 loss 0.005110686179250479\n",
      "Epoch 21/20 Batch 35/68 loss 0.009540345519781113\n",
      "Epoch 21/20 Batch 36/68 loss 0.029551759362220764\n",
      "Epoch 21/20 Batch 37/68 loss 0.006554924882948399\n",
      "Epoch 21/20 Batch 38/68 loss 0.006320341490209103\n",
      "Epoch 21/20 Batch 39/68 loss 0.010203338228166103\n",
      "Epoch 21/20 Batch 40/68 loss 0.055494703352451324\n",
      "Epoch 21/20 Batch 41/68 loss 0.03147253394126892\n",
      "Epoch 21/20 Batch 42/68 loss 0.0022639548406004906\n",
      "Epoch 21/20 Batch 43/68 loss 0.011133304797112942\n",
      "Epoch 21/20 Batch 44/68 loss 0.010338337160646915\n",
      "Epoch 21/20 Batch 45/68 loss 0.0072997878305613995\n",
      "Epoch 21/20 Batch 46/68 loss 0.01569398306310177\n",
      "Epoch 21/20 Batch 47/68 loss 0.012633499689400196\n",
      "Epoch 21/20 Batch 48/68 loss 0.008160087279975414\n",
      "Epoch 21/20 Batch 49/68 loss 0.030955146998167038\n",
      "Epoch 21/20 Batch 50/68 loss 0.013948433101177216\n",
      "Epoch 21/20 Batch 51/68 loss 0.005424639210104942\n",
      "Epoch 21/20 Batch 52/68 loss 0.07077503204345703\n",
      "Epoch 21/20 Batch 53/68 loss 0.018940281122922897\n",
      "Epoch 21/20 Batch 54/68 loss 0.01053297333419323\n",
      "Epoch 21/20 Batch 55/68 loss 0.004106509033590555\n",
      "Epoch 21/20 Batch 56/68 loss 0.03935577720403671\n",
      "Epoch 21/20 Batch 57/68 loss 0.007961017079651356\n",
      "Epoch 21/20 Batch 58/68 loss 0.012565681710839272\n",
      "Epoch 21/20 Batch 59/68 loss 0.008627068251371384\n",
      "Epoch 21/20 Batch 60/68 loss 0.03547678515315056\n",
      "Epoch 21/20 Batch 61/68 loss 0.015403572469949722\n",
      "Epoch 21/20 Batch 62/68 loss 0.007719567511230707\n",
      "Epoch 21/20 Batch 63/68 loss 0.02071213163435459\n",
      "Epoch 21/20 Batch 64/68 loss 0.006073676515370607\n",
      "Epoch 21/20 Batch 65/68 loss 0.0657399520277977\n",
      "Epoch 21/20 Batch 66/68 loss 0.008107506670057774\n",
      "Epoch 21/20 Batch 67/68 loss 0.028018446639180183\n",
      "Epoch 21/20 Batch 68/68 loss 0.03877449780702591\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # print(batch[1])\n",
    "        x, y_true = batch\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, y_true)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs} Batch {idx + 1}/{len(train_dataloader)} loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  / \n",
    "- \n",
    "- precision, recall, f1 score\n",
    "-    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257, 64) (540, 64) (1257,) (540,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.3, random_state=42, stratify=mnist.target)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257\n",
      "540\n"
     ]
    }
   ],
   "source": [
    "class customDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "train_ds = customDataset(x_train, y_train)\n",
    "test_ds = customDataset(x_test, y_test)\n",
    "\n",
    "print(train_ds.__len__())\n",
    "print(test_ds.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'pin_memory_device', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index, batch = next(enumerate(train_dataloader))\n",
    "x, y = batch\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customModel(\n",
      "  (linear): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class customModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customModel, self).__init__()\n",
    "        self.linear = nn.Linear(64, 32)\n",
    "        self.linear2 = nn.Linear(32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(1) # 1   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "model = customModel()\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "def train(model, dataloader, optimizer, loss_func, epochs):\n",
    "    losses = []\n",
    "    #   \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            # \n",
    "            y_pred = model(data)\n",
    "\n",
    "            # \n",
    "            loss = loss_func(y_pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #  \n",
    "            loss.backward()\n",
    "            # \n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch : {epoch}, Loss : {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 1.7008533477783203\n",
      "Epoch : 1, Loss : 1.78902268409729\n",
      "Epoch : 2, Loss : 1.5882059335708618\n",
      "Epoch : 3, Loss : 1.5582761764526367\n",
      "Epoch : 4, Loss : 2.1602888107299805\n",
      "Epoch : 5, Loss : 1.6518681049346924\n",
      "Epoch : 6, Loss : 1.8163636922836304\n",
      "Epoch : 7, Loss : 1.7204962968826294\n",
      "Epoch : 8, Loss : 1.5911978483200073\n",
      "Epoch : 9, Loss : 1.7631276845932007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7008533477783203,\n",
       " 1.78902268409729,\n",
       " 1.5882059335708618,\n",
       " 1.5582761764526367,\n",
       " 2.1602888107299805,\n",
       " 1.6518681049346924,\n",
       " 1.8163636922836304,\n",
       " 1.7204962968826294,\n",
       " 1.5911978483200073,\n",
       " 1.7631276845932007]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_dataloader, optimizer, criterion, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_func, epochs):\n",
    "    losses = []\n",
    "    #   \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            # \n",
    "            y_pred = model(data)\n",
    "            # \n",
    "            loss = loss_func(y_pred, target)\n",
    "\n",
    "        print(epoch, loss.item())\n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model, train_dataloader, test_dataloader, optimizer, loss_func, epochs):\n",
    "    train_losses = []\n",
    "    # train_accuracy = []\n",
    "    validation_losses = []\n",
    "    #   \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_dataloader:\n",
    "            # \n",
    "            y_pred = model(data)\n",
    "\n",
    "            # \n",
    "            loss = loss_func(y_pred, target)\n",
    "            optimizer.zero_grad()\n",
    "            #  \n",
    "            loss.backward()\n",
    "            # \n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch : {epoch}, Train Loss : {loss.item()}')\n",
    "        train_losses.append(loss.item())\n",
    "        # train_accuracy.append(acc)\n",
    "\n",
    "        for data, target in train_dataloader:\n",
    "            # \n",
    "            y_pred = model(data)\n",
    "\n",
    "            # \n",
    "            loss = loss_func(y_pred, target)\n",
    "\n",
    "\n",
    "        print(f'Val Loss : {loss.item()}')\n",
    "        validation_losses.append(loss.item())\n",
    "\n",
    "    return train_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Train Loss : 2.3710451126098633\n",
      "Val Loss : 2.344247817993164\n",
      "Epoch : 1, Train Loss : 2.3867568969726562\n",
      "Val Loss : 2.309196949005127\n",
      "Epoch : 2, Train Loss : 2.3693790435791016\n",
      "Val Loss : 2.304610252380371\n",
      "Epoch : 3, Train Loss : 2.357323169708252\n",
      "Val Loss : 2.3275766372680664\n",
      "Epoch : 4, Train Loss : 2.3765177726745605\n",
      "Val Loss : 2.3341686725616455\n",
      "Epoch : 5, Train Loss : 2.3115053176879883\n",
      "Val Loss : 2.3611385822296143\n",
      "Epoch : 6, Train Loss : 2.312469244003296\n",
      "Val Loss : 2.321190357208252\n",
      "Epoch : 7, Train Loss : 2.324063777923584\n",
      "Val Loss : 2.305835723876953\n",
      "Epoch : 8, Train Loss : 2.3478760719299316\n",
      "Val Loss : 2.3835740089416504\n",
      "Epoch : 9, Train Loss : 2.339662551879883\n",
      "Val Loss : 2.3710224628448486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.3710451126098633,\n",
       "  2.3867568969726562,\n",
       "  2.3693790435791016,\n",
       "  2.357323169708252,\n",
       "  2.3765177726745605,\n",
       "  2.3115053176879883,\n",
       "  2.312469244003296,\n",
       "  2.324063777923584,\n",
       "  2.3478760719299316,\n",
       "  2.339662551879883],\n",
       " [2.344247817993164,\n",
       "  2.309196949005127,\n",
       "  2.304610252380371,\n",
       "  2.3275766372680664,\n",
       "  2.3341686725616455,\n",
       "  2.3611385822296143,\n",
       "  2.321190357208252,\n",
       "  2.305835723876953,\n",
       "  2.3835740089416504,\n",
       "  2.3710224628448486])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validate(model, train_dataloader, test_dataloader, optimizer, criterion, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
